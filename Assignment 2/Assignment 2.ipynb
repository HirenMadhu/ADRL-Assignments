{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gc\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import PIL\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "from skimage import io, transform\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import h5py\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from tqdm import tqdm\n",
    "import idx2numpy\n",
    "from numpyencoder import NumpyEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.pyplot import imshow, imsave\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchsummary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Diffusion</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Supporing Functions and HyperParameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diffusion Hyper Parameters\n",
    "BATCH_SIZE =  100\n",
    "IMG_SIZE = 64\n",
    "device = 'cuda:1'\n",
    "\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as fp:\n",
    "        out = pickle.load(fp)\n",
    "    return out\n",
    "\n",
    "class CelebA_Dataset(Dataset):\n",
    "    def __init__(self, data_list , img_dir, transform=None, target_transform=None):\n",
    "        self.img_titles = data_list\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_titles [idx])\n",
    "        image = PIL.Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "class Bitmoji_Dataset(Dataset):\n",
    "    def __init__(self, path_list , transform=None, target_transform=None):\n",
    "        self.path_list = path_list\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.path_list[idx]\n",
    "        image = PIL.Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "class Bitmoji_Categorical_Dataset(Dataset):\n",
    "    def __init__(self, path_list , transform=None, target_transform=None):\n",
    "        self.path_list = path_list\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path,label = self.path_list[idx]\n",
    "        label = torch.tensor(label,dtype = torch.long)\n",
    "        image = PIL.Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image,label\n",
    "\n",
    "def plot_images(images):\n",
    "    plt.figure(figsize=(32, 32))\n",
    "    plt.imshow(torch.cat([\n",
    "        torch.cat([i for i in images.cpu()], dim=-1),\n",
    "    ], dim=-2).permute(1, 2, 0).cpu())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_images(images, path, **kwargs):\n",
    "    grid = torchvision.utils.make_grid(images, nrow = 10)\n",
    "    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)\n",
    "\n",
    "\n",
    "def get_CelebA_data():\n",
    "    data = pd.read_csv('/home/hiren/Apoorv Pandey/ADRL/Ass1/list_eval_partition.csv')\n",
    "    \n",
    "\n",
    "    train_data,val_data,test_data = data[data['partition']==0]['image_id'].to_list(),data[data['partition']==1]['image_id'].to_list(),\\\n",
    "                                    data[data['partition']==2]['image_id'].to_list()\n",
    "\n",
    "\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    train_dataset = CelebA_Dataset(train_data,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba',transform)\n",
    "    val_dataset = CelebA_Dataset(val_data,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba',transform)\n",
    "    test_dataset = CelebA_Dataset(test_data,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    val_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    return train_loader,val_loader,test_loader\n",
    "\n",
    "\n",
    "def get_Celeb_A_categorical_data():\n",
    "    celeb_attrs = pd.read_csv('/home/hiren/Apoorv Pandey/ADRL/Ass1/list_attr_celeba.csv')\n",
    "    attrs_only = celeb_attrs.loc[:, celeb_attrs.columns != 'image_id']\n",
    "    attrs_only = attrs_only.applymap(lambda x: 1 if x==1 else 0)\n",
    "    attrs_only = attrs_only.sum(axis = 0, skipna = True)\n",
    "    attrs_list = np.array(attrs_only.values.tolist())\n",
    "    \n",
    "    attrs_list = np.argsort(attrs_list)\n",
    "\n",
    "    top_10_attrs = attrs_list[0:10]\n",
    "\n",
    "    path_list = []\n",
    "    for label in top_10_attrs:\n",
    "        path = celeb_attrs[celeb_attrs.iloc[:,label+1] == 1]['image_id'].values.tolist()\n",
    "        #print(len(path))\n",
    "        path_list = path_list + [[p,label] for p in path]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    train_path_list,val_path_list = path_list[0:int(0.8*len(path_list))],path_list[int(0.8*len(path_list)):]\n",
    "    \n",
    "    train_dataset = Bitmoji_Categorical_Dataset(train_path_list,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba',transform)\n",
    "    val_dataset =  Bitmoji_Categorical_Dataset(val_path_list,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba',transform)            \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    return train_loader,val_loader\n",
    "    \n",
    "\n",
    "def get_Bitmoji_data():\n",
    "    \n",
    "    image_path_list = glob.glob('/home/hiren/adrl/1/bitmojis/*.png')\n",
    "    train_images_path,val_images_path = image_path_list[0:int(0.8*len(image_path_list))],image_path_list[int(0.8*len(image_path_list)):]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    train_dataset = Bitmoji_Dataset(train_images_path,transform)\n",
    "    val_dataset = Bitmoji_Dataset(val_images_path,transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    \n",
    "    return train_loader,val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Diffusion Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=device):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = self.noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t,mode = 'train'):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        \n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        if mode == 'eval':\n",
    "            ## Done so that initial image is not corrupted while running denoising steps\n",
    "            if t[0] == self.noise_steps-1: \n",
    "                epsilon = torch.zeros_like(x)\n",
    "                sqrt_alpha_hat = 1.\n",
    "                sqrt_one_minus_alpha_hat = 0.\n",
    "            else:\n",
    "                epsilon = torch.randn_like(x)\n",
    "        \n",
    "        print(sqrt_alpha_hat, sqrt_one_minus_alpha_hat)\n",
    "        \n",
    "\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x\n",
    "    \n",
    "    def sample_at_intervals(self,model,n,t=1000):\n",
    "        model.eval()\n",
    "        stepsize = self.noise_steps//t\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps,stepsize)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Supporting Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional_Block(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,mid_ch=None,residual = False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_ch:\n",
    "            mid_ch = out_ch\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_ch,mid_ch,kernel_size=3,padding = 1,bias=False),\\\n",
    "                                nn.GroupNorm(1, mid_ch),nn.GELU(),\\\n",
    "                                nn.Conv2d(mid_ch,out_ch,kernel_size=3,padding = 1,bias=False),\\\n",
    "                                 nn.GroupNorm(1, out_ch),nn.GELU())\n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.residual:\n",
    "            return nn.GELU()(x + self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            Convolutional_Block(in_channels, in_channels, residual=True),\n",
    "            Convolutional_Block(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            Convolutional_Block(in_channels, in_channels, residual=True),\n",
    "            Convolutional_Block(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=256, device=device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "        self.inc = Convolutional_Block(c_in, 64)\n",
    "        self.down1 = EncoderLayer(64, 128)\n",
    "        self.down2 = EncoderLayer(128, 256)\n",
    "        self.down3 = EncoderLayer(256, 256)\n",
    "\n",
    "\n",
    "        self.bot1 = Convolutional_Block(256, 512)\n",
    "        self.bot2 = Convolutional_Block(512, 512)\n",
    "        self.bot3 = Convolutional_Block(512, 256)\n",
    "\n",
    "        self.up1 = DecoderLayer(512, 128)\n",
    "        self.up2 = DecoderLayer(256, 64)\n",
    "        self.up3 = DecoderLayer(128, 64)\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x4 = self.down3(x3, t)\n",
    "\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.up3(x, x1, t)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_name,num_epochs=100):\n",
    "    if data_name == 'CelebA':\n",
    "        train_loader,_,_ = get_CelebA_data()\n",
    "    else:\n",
    "        train_loader,_ = get_Bitmoji_data()\n",
    "    model = UNet().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    mse = nn.MSELoss()\n",
    "    diffusion = Diffusion(noise_steps = 500,img_size=IMG_SIZE)\n",
    "    writer = SummaryWriter(f'runs/{data_name}/Diffusion/T=500')\n",
    "    l = len(train_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(train_loader)\n",
    "        for i, images in enumerate(pbar):\n",
    "            torch.cuda.empty_cache()\n",
    "            images = images.to(device)\n",
    "            #print(f'Images shape = {images.size()}')\n",
    "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "            \n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            #print(f'x_t shape = {x_t.size()} t shape = {t.size()}' )\n",
    "            predicted_noise = model(x_t, t)\n",
    "            loss = mse(noise, predicted_noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(MSE=loss.item())\n",
    "            writer.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
    "\n",
    "        sampled_images = diffusion.sample(model, n=100)\n",
    "        save_images(sampled_images, f'/home/hiren/Apoorv Pandey/ADRL/Ass2/CelebA_Plots_Diffusion/Epoch_{epoch}.jpg')\n",
    "        torch.save(model.state_dict(),f'/home/hiren/Apoorv Pandey/ADRL/Ass2/Diffusion_Model_{data_name}.pt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train('CelebA',100) ## Training on Celeb A Dataset\n",
    "train('Bitmoji',100) ## Training on Bitmoji Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>FID Calculations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FID(model,path,timesteps = 1000,data_name='CelebA'):\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        diffusion = Diffusion(img_size=IMG_SIZE,device=device)\n",
    "        sampled_images = diffusion.sample_at_intervals(model, 1000,timesteps).type(torch.uint8).to('cpu')\n",
    "        save_object(sampled_images,path)\n",
    "    else:\n",
    "        sampled_images = load_object(path)\n",
    "        \n",
    "    if data_name=='CelebA':\n",
    "        real_loader,_,_ = get_CelebA_data()\n",
    "    else:\n",
    "        real_loader,_ = get_Bitmoji_data()\n",
    "        \n",
    "    real_images = next(iter(real_loader)).to('cpu')\n",
    "    for i in range(9):\n",
    "        curr_batch = next(iter(real_loader)).to('cpu')\n",
    "        real_images = torch.cat((real_images,curr_batch),dim=0)\n",
    "\n",
    "    real_images = real_images.reshape(1000,3,64,64)\n",
    "    real_images = (real_images.clamp(-1, 1) + 1) / 2\n",
    "    real_images = (real_images * 255).type(torch.uint8)\n",
    "    \n",
    "    sampled_images = sampled_images.type(torch.uint8).to('cpu')\n",
    "    fid = FrechetInceptionDistance(feature=2048)\n",
    "    # generate two slightly overlapping image intensity distributions\n",
    "    fid.update(real_images, real=True)\n",
    "    fid.update(sampled_images, real=False)\n",
    "    fid_score = fid.compute()\n",
    "    return fid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_images(model,real_path,fake_path):\n",
    "    diffusion = Diffusion(img_size=IMG_SIZE,device=device)\n",
    "    random_10_samples = diffusion.sample(model, n=10).to('cpu')\n",
    "    save_images(random_10_samples,real_path)\n",
    "    noisy_images = torch.empty((10,10,3,64,64),dtype = torch.uint8)\n",
    "    for i in range(0,10):\n",
    "        t = (torch.ones(10)*(999-i) ).long().to(device)\n",
    "        noisy_image,eps =  diffusion.noise_images(random_10_samples.type(torch.float).to(device), t)\n",
    "        noisy_image = (noisy_image.clamp(-1, 1) + 1) / 2\n",
    "        noisy_image = (noisy_image * 255).type(torch.uint8)\n",
    "        noisy_images[:,i,:,:,:] = noisy_image\n",
    "\n",
    "    noisy_images = noisy_images.view(-1,3,64,64)\n",
    "    save_images(noisy_images,fake_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computing Celeb A FID Scores</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load('/home/hiren/Apoorv Pandey/ADRL/Ass2/Diffusion_Model.pt'))\n",
    "celebA_fid_1000 = get_FID(model,'./Fake_CelebA.pkl',1000)\n",
    "celebA_fid_500 = get_FID(model,'./Fake_CelebA_500.pkl',500)\n",
    "celebA_fid_100 = get_FID(model,'./Fake_CelebA_100.pkl',100)\n",
    "with open('./FID_Scores_Q1.txt','a') as f:\n",
    "    print(f'Time Steps: 1000 Celeb A  : {celebA_fid_1000}',file = f)\n",
    "    print(f'Time Steps: 500 Celeb A  : {celebA_fid_500}',file = f)\n",
    "    print(f'Time Steps: 100 Celeb A  : {celebA_fid_100}',file = f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computing Bitmoji  FID Scores</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load('/home/hiren/Apoorv Pandey/ADRL/Ass2/Diffusion_Model_Bitmoji.pt'))\n",
    "bitmoji_fid_1000 = get_FID(model,'./Fake_Bitmoji.pkl',1000,'Bitmoji')\n",
    "bitmoji_fid_500 = get_FID(model,'./Fake_Bitmoji_500.pkl',500,'Bitmoji')\n",
    "bitmoji_fid_100 = get_FID(model,'./Fake_Bitmoji_100.pkl',100,'Bitmoji')\n",
    "with open('./FID_Scores_Q1.txt','a') as f:\n",
    "    print(f'Time Steps: 1000 Bitmoji : {bitmoji_fid_1000}',file = f)\n",
    "    print(f'Time Steps: 500 Bitmoji : {bitmoji_fid_500}',file = f)\n",
    "    print(f'Time Steps: 100 Bitmoji : {bitmoji_fid_100}',file = f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sampling 100 generated celeb A images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_images_1000 = load_object('./Fake_CelebA.pkl')[0:100]\n",
    "sampled_images_500 = load_object('./Fake_CelebA_500.pkl')[0:100]\n",
    "sampled_images_100 = load_object('./Fake_CelebA_100.pkl')[0:100]\n",
    "\n",
    "save_images(sampled_images_1000,'./Fake_CelebA_1000.png')\n",
    "save_images(sampled_images_500,'./Fake_CelebA_500.png')\n",
    "save_images(sampled_images_100,'./Fake_CelebA_100.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sampling 100 generated BitMoji images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_images_1000 = load_object('./Fake_Bitmoji.pkl')[0:100]\n",
    "sampled_images_500 = load_object('./Fake_Bitmoji_500.pkl')[0:100]\n",
    "sampled_images_100 = load_object('./Fake_Bitmoji_100.pkl')[0:100]\n",
    "\n",
    "save_images(sampled_images_1000,'./Fake_Bitmoji_1000.png')\n",
    "save_images(sampled_images_500,'./Fake_Bitmoji_500.png')\n",
    "save_images(sampled_images_100,'./Fake_Bitmoji_100.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generate Denoising Images for Celeb A and Bitmoji</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load('/home/hiren/Apoorv Pandey/ADRL/Ass2/Diffusion_Model.pt'))\n",
    "\n",
    "generate_noisy_images(model,'./CelebA_10_images.jpg','./CelebA_noisy_Images.jpg')\n",
    "\n",
    "\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load('/home/hiren/Apoorv Pandey/ADRL/Ass2/Diffusion_Model_Bitmoji.pt'))\n",
    "\n",
    "generate_noisy_images(model,'./BItmoji_10_images.jpg','./Bitmoji_noisy_Images.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classifier Guidance</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters and Supporting functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "T = 1500\n",
    "\n",
    "class CelebA_Categorical_Dataset(Dataset):\n",
    "    def __init__(self, img_titles ,img_dir, transform=None, target_transform=None):\n",
    "        self.img_titles = img_titles\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path,label = os.path.join(self.img_dir, self.img_titles[idx][0]),self.img_titles[idx][1]\n",
    "        label = torch.tensor(label,dtype = torch.long)\n",
    "        image = PIL.Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image,label\n",
    "\n",
    "def get_Celeb_A_categorical_data():\n",
    "    celeb_attrs = pd.read_csv('/home/hiren/Apoorv Pandey/ADRL/Ass1/list_attr_celeba.csv')\n",
    "    attrs_only = celeb_attrs.loc[:, celeb_attrs.columns != 'image_id']\n",
    "    attrs_only = attrs_only.applymap(lambda x: 1 if x==1 else 0)\n",
    "    attrs_only.to_csv('/home/hiren/Apoorv Pandey/ADRL/Ass1/attrs_only.csv')\n",
    "    attrs_only = attrs_only.sum(axis = 0, skipna = True)\n",
    "    #print(attrs_only)\n",
    "    attrs_list = np.array(attrs_only.values.tolist())\n",
    "    \n",
    "    attrs_list = np.argsort(attrs_list)\n",
    "    #print(attrs_list)\n",
    "    top_10_attrs = attrs_list[0:10]\n",
    "    top_10_attrs_names = []\n",
    "    for attr in top_10_attrs:\n",
    "        top_10_attrs_names.append(celeb_attrs.columns[attr+1])\n",
    "    print(f'Selected attributes : {top_10_attrs_names}')\n",
    "    path_list = []\n",
    "    for i,label in enumerate(top_10_attrs):\n",
    "        path = celeb_attrs[celeb_attrs.iloc[:,label+1] == 1]['image_id'].values.tolist()\n",
    "        #print(len(path))\n",
    "        path_list = path_list + [[p,i] for p in path]  ## pass labels in range 0 to 9 with 0 being most frequent one\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    train_path_list,val_path_list = path_list[0:int(0.8*len(path_list))],path_list[int(0.8*len(path_list)):]\n",
    "    #print(train_path_list)\n",
    "    train_dataset = CelebA_Categorical_Dataset(train_path_list,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba',transform)\n",
    "    val_dataset =  CelebA_Categorical_Dataset(val_path_list,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba',transform)            \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    return train_loader,val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conditional Diffusion</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Conditional_Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.beta = self.noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        epsilon = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n,labels,gamma=3):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(device)\n",
    "                predicted_noise = model(x, t,labels)\n",
    "                if gamma > 0:\n",
    "                    uncond_predicted_noise = model(x, t, None)\n",
    "                    predicted_noise = uncond_predicted_noise + gamma*(predicted_noise - uncond_predicted_noise)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x\n",
    "\n",
    "class UNet_Conditional(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=256, num_classes = None):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.inc = Convolutional_Block(c_in, 64)\n",
    "        self.down1 = EncoderLayer(64, 128)\n",
    "        self.down2 = EncoderLayer(128, 256)\n",
    "        self.down3 = EncoderLayer(256, 256)\n",
    "\n",
    "        self.bot1 = Convolutional_Block(256, 512)\n",
    "        self.bot2 = Convolutional_Block(512, 512)\n",
    "        self.bot3 = Convolutional_Block(512, 256)\n",
    "\n",
    "        self.up1 = DecoderLayer(512, 128)\n",
    "        self.up2 = DecoderLayer(256, 64)\n",
    "        self.up3 = DecoderLayer(128, 64)\n",
    "\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "        \n",
    "        if self.num_classes is not None:\n",
    "            self.label_emb = nn.Embedding(num_classes,time_dim)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t,labels):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        if labels is not None:\n",
    "            t += self.label_emb(labels)\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x4 = self.down3(x3, t)\n",
    "\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t)\n",
    "\n",
    "        x = self.up2(x, x2, t)\n",
    "\n",
    "        x = self.up3(x, x1, t)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,val_loader = get_Celeb_A_categorical_data()\n",
    "model = UNet_Conditional(num_classes=10).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "mse = nn.MSELoss()\n",
    "diffusion = Conditional_Diffusion(img_size=IMG_SIZE)\n",
    "output_file = './UNet_Conditional_logs.txt'\n",
    "logger = SummaryWriter('/home/hiren/Apoorv Pandey/ADRL/Ass2/runs/CelebA_Conditional')\n",
    "l = len(train_loader)\n",
    "#model.load_state_dict(torch.load('/home/hiren/Apoorv Pandey/ADRL/Ass2/Diffusion_Model_CelebA_Conditional.pt'))\n",
    "for epoch in range(100):\n",
    "    pbar = tqdm(train_loader)\n",
    "    for i, (images,labels) in enumerate(pbar):\n",
    "        torch.cuda.empty_cache()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #print(f'Images shape = {images.size()}')\n",
    "        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "        if np.random.random()<0.1:  ## 10% of time use unconditional generation\n",
    "            labels = None\n",
    "        x_t, noise = diffusion.noise_images(images, t)\n",
    "        #print(f'x_t shape = {x_t.size()} t shape = {t.size()}' )\n",
    "        predicted_noise = model(x_t, t,labels)\n",
    "        loss = mse(noise, predicted_noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(MSE=loss.item())\n",
    "        logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
    "        with open(output_file, 'a') as f:\n",
    "            print(f'Epoch :{epoch}  MSE : {loss.item()} global_step: {epoch * l + i}', file=f) \n",
    "    \n",
    "    class_labels = [[i]*10 for i in range(10)]\n",
    "    class_labels = torch.tensor(class_labels,dtype = torch.long).to(device)\n",
    "    class_labels = class_labels.reshape(-1)\n",
    "    sampled_images = diffusion.sample(model, n=100,labels = class_labels)\n",
    "    save_images(sampled_images, f'/home/hiren/Apoorv Pandey/ADRL/Ass2/CelebA_Categorical/Epoch_{epoch}.jpg')\n",
    "    torch.save(model.state_dict(),'/home/hiren/Apoorv Pandey/ADRL/Ass2/Diffusion_Model_CelebA_Conditional.pt' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Domain Adaptation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Basic ResNet</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uspsDataset(Dataset):\n",
    "    def __init__(self, path, tr = True):\n",
    "        \n",
    "        with h5py.File(path, 'r') as hf:\n",
    "            if(tr):\n",
    "                train = hf.get('train')\n",
    "                x = train.get('data')[:].reshape(7291,16,16)\n",
    "                self.X = np.array([transform.resize(i, (32, 32), anti_aliasing=True) for i in x])\n",
    "                self.y = train.get('target')[:]\n",
    "            else:\n",
    "                test = hf.get('test')\n",
    "                x = test.get('data')[:].reshape(2007,16,16)\n",
    "                self.X = np.array([transform.resize(i, (32, 32), anti_aliasing=True) for i in x])\n",
    "                self.y = test.get('target')[:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistDataset(Dataset):\n",
    "    def __init__(self, path, tr = True):\n",
    "            if(tr):\n",
    "                x = idx2numpy.convert_from_file('mnist/train-images.idx3-ubyte')\n",
    "                self.X = np.array([transform.resize(i, (32, 32), anti_aliasing=True) for i in x])\n",
    "                #self.X = transform.resize(idx2numpy.convert_from_file('mnist/train-images.idx3-ubyte'), (60000, 1, 32, 32), anti_aliasing=True)\n",
    "                self.y = idx2numpy.convert_from_file('mnist/train-labels.idx1-ubyte')\n",
    "            else:\n",
    "                x = idx2numpy.convert_from_file('mnist/t10k-images.idx3-ubyte')\n",
    "                self.X = np.array([transform.resize(i, (32, 32), anti_aliasing=True) for i in x])\n",
    "                #self.X = transform.resize(idx2numpy.convert_from_file('mnist/t10k-images.idx3-ubyte'), (10000, 1, 32, 32), anti_aliasing=True)\n",
    "                self.y = idx2numpy.convert_from_file('mnist/t10k-labels.idx1-ubyte')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clipRealData(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.X = np.load(path+'/X.npy', allow_pickle=True)\n",
    "        self.y = np.load(path+'/y.npy', allow_pickle=True)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = stride, padding = 0),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 2, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 1, stride = stride, padding = 0),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.skipconv = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1, bias=False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        try:\n",
    "            out = self.conv2(out)\n",
    "            out = self.conv3(out)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        res = self.bn(self.skipconv(x))\n",
    "        return out+res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, imsize, num_classes = 10, mnist = True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.imszie = imsize\n",
    "        self.mnist = mnist\n",
    "        if mnist:\n",
    "            self.layer1 = ResidualBlock(1, 2**4)\n",
    "        else:\n",
    "            self.layer1 = ResidualBlock(3, 2**4)\n",
    "        self.layer2 = ResidualBlock(2**4, 2**5)\n",
    "        self.layer3 = ResidualBlock(2**5, 2**6)\n",
    "        self.layer4 = ResidualBlock(2**6, 2**7)\n",
    "        self.layer5 = ResidualBlock(2**7, 2**8)\n",
    "        self.layer6 = ResidualBlock(2**8, 2**9)\n",
    "        self.layer7 = ResidualBlock(2**9, 2**10)\n",
    "        self.imsize = imsize\n",
    "        if(imsize==64):\n",
    "            self.fc = nn.Linear(2**9, num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(2**8, num_classes)\n",
    "    def forward(self, x):\n",
    "        bsize = x.shape[0]\n",
    "        if self.mnist:\n",
    "            x = x.view(bsize,1,self.imsize, self.imsize)\n",
    "        else:\n",
    "            x = x.view(bsize,3,self.imsize, self.imsize)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        if(self.imsize==64):\n",
    "            x = self.layer6(x)\n",
    "        return F.softmax(self.fc(x.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data, bsize=3000, path=None):\n",
    "    dataloader = DataLoader(data, bsize)\n",
    "    true = torch.empty(0).to(device)\n",
    "    pred = torch.empty(0).to(device)\n",
    "    for ip,t in dataloader:\n",
    "        #ip = torch.cuda.FloatTensor(data.X)\n",
    "        #true = data.y#.cpu().detach().numpy()\n",
    "        op = model(ip.float().to(device))\n",
    "        p = torch.argmax(op, axis = 1)\n",
    "        true = torch.cat((true, t.to(device)), dim=0)\n",
    "        pred = torch.cat((pred, p.to(device)), dim=0)\n",
    "        del(op, ip)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    true = true.cpu().detach().numpy()\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "    if path:\n",
    "        x = {'acc' : accuracy_score(true, pred), 'f1' : precision_recall_fscore_support(true, pred, average='macro')[2], 'misses':list(np.where(pred!=true)[0])}\n",
    "        y = json.dumps(x, indent=4, cls=NumpyEncoder)\n",
    "        with open(path, 'w') as outfile:\n",
    "            outfile.write(y)\n",
    "    \n",
    "    return accuracy_score(true, pred), precision_recall_fscore_support(true, pred, average='macro')[2], np.where(pred!=true)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cycleGAN(model, gen, data, bsize=256, path=None):\n",
    "    dataloader = DataLoader(data, bsize)\n",
    "    true = torch.empty(0).to(device)\n",
    "    pred = torch.empty(0).to(device)\n",
    "    for ip,t in dataloader:\n",
    "        #ip = torch.cuda.FloatTensor(data.X)\n",
    "        #true = data.y#.cpu().detach().numpy()\n",
    "        ip = gen(ip.float().cuda())\n",
    "        op = model(ip.float().to(device))\n",
    "        p = torch.argmax(op, axis = 1)\n",
    "        true = torch.cat((true, t.to(device)), dim=0)\n",
    "        pred = torch.cat((pred, p.to(device)), dim=0)\n",
    "        del(op, ip)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    true = true.cpu().detach().numpy()\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "    \n",
    "    return accuracy_score(true, pred), precision_recall_fscore_support(true, pred, average='macro')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 369\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "params = {\n",
    "    \"bsize\" : 5000,# Batch size during training.\n",
    "    'imsize' : 32,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nepochs' : 15,#Number of training epochs.\n",
    "    'lr' : 0.002,#Learning rate for optimizers\n",
    "    'nclasses' : 10, #number of classes\n",
    "    'save_epoch' : 20}# Save step.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "uspsTrain = uspsDataset('usps/usps.h5')\n",
    "uspsTrainLoader = DataLoader(uspsTrain, params['bsize'], shuffle=True)\n",
    "uspsTest = uspsDataset('usps/usps.h5', False)\n",
    "uspsTestLoader = DataLoader(uspsTest, params['bsize'], shuffle=True)\n",
    "mnistTrain = mnistDataset('')\n",
    "mnistTrainLoader = DataLoader(mnistTrain, params['bsize'], shuffle=True)\n",
    "mnistTest = mnistDataset('', False)\n",
    "mnistTestLoader = DataLoader(mnistTest, params['bsize'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(params['imsize'], params['nclasses'], True).to(device)\n",
    "model.apply(weights_init)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])  \n",
    "train_loss, domain_accuracy, cross_domain_accuracy = [],[],[]\n",
    "for epoch in range(params['nepochs']):\n",
    "    for images, labels in uspsTrainLoader:  \n",
    "        images = images.to(device)\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    domain_accuracy.append(eval(model, uspsTest)[0])\n",
    "    cross_domain_accuracy.append(eval(model, mnistTrain)[0])\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}, Test accuracy : {:.4}' \n",
    "                   .format(epoch+1, params['nepochs'], loss.item(), domain_accuracy[-1]))\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "eval(model, mnistTrain, path='output/mnistTrain.json')\n",
    "eval(model, mnistTest, path='output/mnistTest.json')\n",
    "torch.save({\n",
    "            'model' : model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'output/trained_on_usps_resnet.pth')\n",
    "\n",
    "del(model, optimizer)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), np.array(train_loss))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.title(\"Iteration vs loss(resnet trained on USPS)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(domain_accuracy)), np.array(domain_accuracy),color='r', label='Domain Accuracy')\n",
    "plt.plot(np.arange(len(domain_accuracy)), np.array(cross_domain_accuracy), color='g', label='Cross Domainn Accuracy')\n",
    "  \n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Epochs vs Accuracy(ResNet trained on USPS)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(params['imsize'], params['nclasses'], True).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())  \n",
    "train_loss, domain_accuracy, cross_domain_accuracy = [],[],[]\n",
    "for epoch in range(params['nepochs']):\n",
    "    for images, labels in mnistTrainLoader:  \n",
    "        images = images.float().to(device)\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        domain_accuracy.append(eval(model, mnistTest)[0])\n",
    "        cross_domain_accuracy.append(eval(model, uspsTrain)[0])\n",
    "\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}, Test accuracy : {:.4}' \n",
    "                   .format(epoch+1, params['nepochs'], loss.item(), domain_accuracy[-1]))\n",
    "\n",
    "eval(model, uspsTrain, path='output/uspsTrain.json')\n",
    "eval(model, uspsTest, path='output/uspsTest.json')\n",
    "torch.save({\n",
    "            'model' : model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'output/trained_on_mnist_resnet.pth')\n",
    "del(model, optimizer, mnistTest, mnistTrain, mnistTrainLoader, mnistTestLoader, uspsTest, uspsTrain, uspsTrainLoader, uspsTestLoader)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), np.array(train_loss))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Iteration vs loss(resnet trained on MNIST)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(domain_accuracy)), np.array(domain_accuracy),color='r', label='Domain Accuracy')\n",
    "plt.plot(np.arange(len(domain_accuracy)), np.array(cross_domain_accuracy), color='g', label='Cross Domainn Accuracy')\n",
    "  \n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Epochs vs Accuracy(ResNet Trained on MNINST)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 369\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "params = {\n",
    "    \"bsize\" : 256,# Batch size during training.\n",
    "    'imsize' : 64,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nepochs' : 50,#Number of training epochs.\n",
    "    'lr' : 0.002,#Learning rate for optimizers\n",
    "    'nclasses' : 65, #number of classes\n",
    "    'save_epoch' : 20}# Save step.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "clipData = clipRealData('Clipart')\n",
    "realData = clipRealData('Real World')\n",
    "realLoader = DataLoader(realData, batch_size=params['bsize'])\n",
    "clipLoader = DataLoader(clipData, batch_size=params['bsize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(params['imsize'], params['nclasses'], False).to(device)\n",
    "model.apply(weights_init)\n",
    "\n",
    "train_loss, domain_accuracy, cross_domain_accuracy = [],[],[]\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], weight_decay = 0.001, momentum = 0.9)  \n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    for images, labels in clipLoader:  \n",
    "        images = images.float().to(device)\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        cross_domain_accuracy.append(eval(model, realData)[0])\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, params['nepochs'], loss.item()))\n",
    "\n",
    "eval(model, realData,params['bsize'], path = 'output/real.json')\n",
    "torch.save({\n",
    "            'model' : model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'output/trained_on_clip_resnet.pth')\n",
    "\n",
    "del(model, optimizer)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), np.array(train_loss))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Iteration vs loss(resnet trained on Clipart)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(cross_domain_accuracy)), np.array(cross_domain_accuracy))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs loss(resnet trained on Clipart)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(params['imsize'], params['nclasses'], False).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], weight_decay = 0.001, momentum = 0.9)  \n",
    "train_loss, domain_accuracy, cross_domain_accuracy = [],[],[]\n",
    "for epoch in range(params['nepochs']):\n",
    "    for images, labels in realLoader:  \n",
    "        images = images.float().to(device)\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        train_loss.append(loss.item())\n",
    "        cross_domain_accuracy.append(eval(model, clipData)[0])\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, params['nepochs'], loss.item()))\n",
    "\n",
    "eval(model, clipData, path = 'output/real.json')\n",
    "torch.save({\n",
    "            'model' : model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'output/trained_on_clip_resnet.pth')\n",
    "\n",
    "del(model, optimizer)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), np.array(train_loss))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Iteration vs loss(resnet trained on Real World)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(cross_domain_accuracy)), np.array(cross_domain_accuracy))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs loss(resnet trained on Real World)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DANN</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Supporting Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USPS_Dataset(Dataset):\n",
    "    def __init__(self, path ,transform=None, target_transform=None):\n",
    "        with h5py.File(path, 'r') as hf:\n",
    "            train = hf.get('train')\n",
    "            X_tr = train.get('data')[:]\n",
    "            y_tr = train.get('target')[:]\n",
    "            test = hf.get('test')\n",
    "            X_te = test.get('data')[:]\n",
    "            y_te = test.get('target')[:]\n",
    "        X_tr = X_tr.reshape(-1,1,16,16)\n",
    "        X_te = X_te.reshape(-1,1,16,16)\n",
    "        X_usps = np.concatenate([X_tr,X_te],axis=0)\n",
    "        y_usps = np.concatenate([y_tr,y_te],axis=0)\n",
    "        self.data = torch.tensor(X_usps,dtype = torch.float)\n",
    "        self.labels = torch.tensor(y_usps,dtype = torch.long)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "class OfficeHomeDataset(Dataset):\n",
    "    def __init__(self, img_titles ,labels, transform=None, target_transform=None):\n",
    "        self.img_titles = img_titles\n",
    "        self.transform = transform\n",
    "        self.labels = torch.tensor(labels,dtype = torch.long)\n",
    "        \n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_titles[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = PIL.Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image,label\n",
    "\n",
    "def usps_dataloader():\n",
    "    transform = transforms.Compose([transforms.Grayscale(1),transforms.Resize((16,16)),transforms.ToTensor(),\\\n",
    "                                    transforms.Lambda(lambda x: x.repeat(3, 1, 1) ),\\\n",
    "                                    transforms.Normalize((0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
    "    usps_dataset = USPS_Dataset('/home/hiren/Apoorv Pandey/ADRL/Ass2/usps.h5',transform = transform)\n",
    "    usps_dataloader = DataLoader(usps_dataset,batch_size=batch_size,shuffle=True)\n",
    "    return usps_dataloader\n",
    "\n",
    "def mnist_dataloader():\n",
    "    transform = transforms.Compose([transforms.Grayscale(1),transforms.Resize((16,16)),transforms.ToTensor(),\\\n",
    "                                    transforms.Lambda(lambda x: x.repeat(3, 1, 1) ),\\\n",
    "                                    transforms.Normalize((0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
    "\n",
    "    mnist_dataset = MNIST('./Mnist',train=True,download=True,transform=transform)\n",
    "    mnist_loader = DataLoader(mnist_dataset,batch_size = batch_size,shuffle=True)\n",
    "    return mnist_loader\n",
    "\n",
    "    \n",
    "def office_dataloader():\n",
    "    office_dataset_directory = '/home/hiren/Apoorv Pandey/ADRL/Ass2/OfficeHomeDataset_10072016'\n",
    "\n",
    "\n",
    "    real_world_imgs = []\n",
    "\n",
    "    real_world_path = '/home/hiren/Apoorv Pandey/ADRL/Ass2/OfficeHomeDataset_10072016/Real World/'\n",
    "    clipart_path = \"/home/hiren/Apoorv Pandey/ADRL/Ass2/OfficeHomeDataset_10072016/Clipart/\"\n",
    "    category_mapping = {}\n",
    "    for i,x in enumerate(os.walk(real_world_path)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        category = x[0]\n",
    "        if category.startswith(real_world_path):\n",
    "            category = category.replace(real_world_path, '', 1)\n",
    "        category_mapping[category] = i\n",
    "\n",
    "\n",
    "    print(category_mapping)\n",
    "    real_img_list,real_img_labels = [],[]\n",
    "    for i,x in enumerate(os.walk(real_world_path)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        category = x[0]\n",
    "        if category.startswith(real_world_path):\n",
    "            category = category.replace(real_world_path, '', 1)\n",
    "        label = category_mapping[category]\n",
    "\n",
    "        real_img_list =  real_img_list + glob.glob(real_world_path + category+'/*.jpg')\n",
    "        real_img_labels = real_img_labels + [i]*len(real_img_list)\n",
    "\n",
    "\n",
    "\n",
    "    clip_img_list,clip_img_labels = [],[]\n",
    "\n",
    "    for i,x in enumerate(os.walk(clipart_path)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        category = x[0]\n",
    "        if category.startswith(clipart_path):\n",
    "            category = category.replace(clipart_path, '', 1)\n",
    "        label = category_mapping[category]\n",
    "\n",
    "        clip_img_list =  clip_img_list + glob.glob(clipart_path + category+'/*.jpg')\n",
    "\n",
    "        label = category_mapping[category]\n",
    "        clip_img_labels = clip_img_labels + [label]*len(clip_img_list)\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,0.5,0.5),\n",
    "                             std=(0.5,0.5,0.5))\n",
    "    ])\n",
    "    real_office_dataset = OfficeHomeDataset(real_img_list,real_img_labels,transform)\n",
    "    clipart_dataset = OfficeHomeDataset(clip_img_list,clip_img_labels,transform)\n",
    "    \n",
    "    realworld_loader = DataLoader(real_office_dataset,batch_size = batch_size,shuffle=True)\n",
    "    clipart_loader = DataLoader(clipart_dataset,batch_size = batch_size,shuffle=True)\n",
    "    \n",
    "    return realworld_loader,clipart_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "\n",
    "## Basic ResNet50 Classifier\n",
    "class BaseClassifier(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.model.fc = nn.Sequential(nn.Linear(2048,512),nn.ReLU(),nn.Linear(512,num_classes))\n",
    "  \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class FeatureExtractor1(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.model.children())[:-1])\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.feature_extractor(x).squeeze(2).squeeze(2)\n",
    "        #print(f'size of x = {x.size()}')\n",
    "        return x\n",
    "  \n",
    "class FeatureExtractor2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    self.feature_extractor = nn.Sequential(*list(self.model.children())[:-3])\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = self.feature_extractor(x).squeeze(2).squeeze(2)\n",
    "    #print(f'size of x = {x.size()}')\n",
    "    return x\n",
    "\n",
    "class FeatureExtractor3(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    self.feature_extractor = nn.Sequential(*list(self.model.children())[:-4])\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = self.feature_extractor(x).squeeze(2).squeeze(2)\n",
    "    #print(f'size of x = {x.size()}')\n",
    "    return x\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "  def __init__(self,in_features,num_classes=10):\n",
    "    super().__init__()\n",
    "    self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "  def forward(self,x):\n",
    "    \n",
    "    return self.classifier(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_features,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x,alpha):\n",
    "        reversed_x = GradientReversalLayer.apply(x,alpha)\n",
    "        return self.discriminator(reversed_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Basic ResNet50 \n",
    "def base_classifier_train(src_loader,tgt_loader,model,optimizer,writer,checkpoint_path,text_file):\n",
    "    \n",
    "    n_epochs = 30\n",
    "    global_step = 0\n",
    "    best_acc = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        avg_loss,steps = 0.0,0\n",
    "        correct_src,num_samples_src = 0.0,0\n",
    "        \n",
    "        for idx,src_data in enumerate(src_loader):\n",
    "            model.train()\n",
    "            src_x,src_y = src_data\n",
    "            src_x ,src_y= src_x.to(device),src_y.to(device)\n",
    "            \n",
    "            if src_x.size(1)==1:\n",
    "                src_x = torch.cat([src_x,src_x,src_x],dim=1)\n",
    "            \n",
    "            src_batch_len = src_x.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(src_x)\n",
    "            preds = torch.argmax(out,dim=1)\n",
    "            correct_src += torch.sum(preds==src_y)\n",
    "            num_samples_src += src_y.size(0)\n",
    "            loss = criterion(out,src_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            steps += 1\n",
    "            writer.add_scalar('loss',loss.item(),global_step)\n",
    "        avg_loss = avg_loss/steps\n",
    "        avg_accuracy_src = correct_src/num_samples_src\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            correct_tgt,num_samples_tgt = 0.0,0\n",
    "            for idx,tgt_data in enumerate(tgt_loader):\n",
    "\n",
    "                tgt_x,tgt_y = tgt_data\n",
    "                tgt_batch_len = tgt_x.size(0)\n",
    "                if tgt_x.size(1)==1:\n",
    "                    tgt_x = torch.cat([tgt_x,tgt_x,tgt_x],dim=1)\n",
    "                tgt_x,tgt_y = tgt_x.to(device),tgt_y.to(device)\n",
    "                out = model(tgt_x)\n",
    "                preds = torch.argmax(out,dim=1)\n",
    "                correct_tgt += torch.sum(preds==tgt_y)\n",
    "                num_samples_tgt += tgt_y.size(0)\n",
    "            avg_accuracy_tgt = correct_tgt/num_samples_tgt\n",
    "            with open(text_file, 'a') as f:\n",
    "                print(f'Epoch :{epoch}  Src Accuracy = {avg_accuracy_src} Tgt accuracy : {avg_accuracy_tgt} loss: {avg_loss} ', file=f) \n",
    "        \n",
    "\n",
    "        if best_acc < avg_accuracy_tgt:\n",
    "            best_acc = avg_accuracy_tgt\n",
    "            torch.save(model.state_dict(),checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(src_loader,tgt_loader,F,C,D,optimizer,writer,checkpoint_path,text_file):\n",
    "    n_epochs = 30\n",
    "    step = 0\n",
    "    best_acc = 0.0\n",
    "    criterion_c = nn.CrossEntropyLoss()\n",
    "    criterion_d = nn.CrossEntropyLoss()\n",
    "    for epoch in range(n_epochs):\n",
    "        start_steps = epoch * len(src_loader)\n",
    "        total_steps = n_epochs * len(tgt_loader)\n",
    "        avg_c_loss,avg_d_loss,avg_acc = 0.0,0.0,0.0\n",
    "        c_steps,d_steps,acc_steps = 0,0,0\n",
    "        for idx,(src_data,tgt_data) in enumerate(zip(src_loader,tgt_loader)):\n",
    "            F.train(),C.train(),D.train()\n",
    "            \n",
    "            src_x,src_y = src_data\n",
    "            tgt_x,tgt_y = tgt_data\n",
    "            src_batch_len = src_x.size(0)\n",
    "            tgt_batch_len = tgt_x.size(0)\n",
    "            if src_x.size(1)==1:\n",
    "                src_x = torch.cat([src_x,src_x,src_x],dim=1)\n",
    "            if tgt_x.size(1)==1:\n",
    "                tgt_x = torch.cat([tgt_x,tgt_x,tgt_x],dim=1)\n",
    "            #print(f'Src shape = {src_x.size()} Tgt shape {tgt_x.size()}')\n",
    "            src_x ,src_y= src_x.to(device),src_y.to(device)\n",
    "            tgt_x,tgt_y = tgt_x.to(device),tgt_y.to(device)\n",
    "            p = float(idx + start_steps) / total_steps\n",
    "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            \n",
    "            combined_x = torch.cat([src_x,tgt_x],dim=0)\n",
    "            combined_features = F(combined_x)\n",
    "            if combined_features.dim()>2:\n",
    "                combined_features = combined_features.reshape(src_batch_len+tgt_batch_len,-1)\n",
    "            src_features = combined_features[0:len(src_x)]\n",
    "            tgt_features = combined_features[len(src_x):]\n",
    "            \n",
    "            #print(f'Src features shape {src_features.size()}')\n",
    "            \n",
    "\n",
    "            class_pred_src = C(src_features)\n",
    "            c_loss = criterion_c(class_pred_src,src_y)\n",
    "\n",
    "\n",
    "            src_labels = torch.ones(src_batch_len,dtype=torch.long).to(device)\n",
    "            tgt_labels = torch.zeros(tgt_batch_len,dtype=torch.long).to(device)\n",
    "            combined_labels = torch.cat([src_labels,tgt_labels],dim=0)\n",
    "            disc_pred = D(combined_features,alpha)\n",
    "            disc_pred_labels = torch.argmax(disc_pred,dim=1)\n",
    "            d_loss = criterion_d(disc_pred,combined_labels)\n",
    "\n",
    "            #c_loss.backward(retain_graph=True)\n",
    "\n",
    "            #d_loss.backward()\n",
    "            total_loss = d_loss + c_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            avg_c_loss += c_loss.item()\n",
    "            c_steps += src_y.size(0)\n",
    "            avg_d_loss += d_loss.item()\n",
    "            d_steps = combined_labels.size(0)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            writer.add_scalar('C loss',c_loss.item(),step)\n",
    "            writer.add_scalar('D loss',d_loss.item(),step)\n",
    "            writer.add_scalar('Total Loss',total_loss.item(),step)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                F.eval()\n",
    "                C.eval()\n",
    "                \n",
    "                class_pred_tgt = C(tgt_features)\n",
    "                tgt_labels = torch.argmax(class_pred_tgt,dim=1)\n",
    "\n",
    "                correct = torch.sum(tgt_labels==tgt_y)\n",
    "                accuracy = correct/tgt_labels.size(0)\n",
    "                writer.add_scalar('Accuracy',accuracy.item(),step)\n",
    "                avg_acc += correct\n",
    "                acc_steps += tgt_y.size(0)\n",
    "\n",
    "            step += 1\n",
    "            #print(f'Epoch :{epoch} idx :{idx} accuracy : {accuracy} C_loss: {c_loss} d_loss :{d_loss}')\n",
    "            state = {'F':F.state_dict(),'C':C.state_dict,'D':D.state_dict(),'opt':optimizer}\n",
    "        avg_acc = avg_acc/acc_steps\n",
    "        avg_c_loss = avg_c_loss/c_steps\n",
    "        avg_d_loss = avg_d_loss/d_steps\n",
    "        \n",
    "        with open(text_file, 'a') as f:\n",
    "            print(f'Epoch :{epoch}  accuracy : {avg_acc} C_loss: {avg_c_loss} d_loss :{avg_d_loss} ', file=f) \n",
    "        \n",
    "\n",
    "        if best_acc < avg_acc:\n",
    "            best_acc = avg_acc\n",
    "            torch.save(state,checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usps_loader = usps_dataloader()\n",
    "mnist_loader = mnist_dataloader()\n",
    "realworld_loader,clipart_loader = office_dataloader()\n",
    "\n",
    "writer = SummaryWriter('runs/Base/USPS-MNIST')\n",
    "model = BaseClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()))\n",
    "base_classifier_train(usps_loader,mnist_loader,model,optimizer,writer,'./Base_USPS-MNIST.pt','./Base_USPS-MNIST.txt')\n",
    "\n",
    "writer = SummaryWriter('runs/Base/MNIST-USPS')\n",
    "model = BaseClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()))\n",
    "base_classifier_train(mnist_loader,usps_loader,model,optimizer,writer,'./Base_MNIST-USPS.pt','./Base_MNIST-USPS.txt')\n",
    "\n",
    "writer = SummaryWriter('runs/Base/Real-Clipart')\n",
    "model = BaseClassifier(num_classes=65).to(device)\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()))\n",
    "base_classifier_train(usps_loader,mnist_loader,model,optimizer,writer,'./Base_Real-Clipart.pt','./Base_Real-Clipart.txt')\n",
    "\n",
    "writer = SummaryWriter('runs/Base/Clipart-Real')\n",
    "model = BaseClassifier(num_classes=65).to(device)\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()))\n",
    "base_classifier_train(usps_loader,mnist_loader,model,optimizer,writer,'./Base_Clipart-Realt.pt','./Base_Clipart-Real.txt')\n",
    "\n",
    "writer1 = SummaryWriter('runs/DANN/USPS-MNIST/1')\n",
    "writer2 = SummaryWriter('runs/DANN/USPS-MNIST/2')\n",
    "writer3 = SummaryWriter('runs/DANN/USPS-MNIST/3')\n",
    "F1 = FeatureExtractor1().to(device)\n",
    "F2 = FeatureExtractor2().to(device)\n",
    "F3 = FeatureExtractor3().to(device)\n",
    "C1= Classifier(2048,10).to(device)\n",
    "C2 = Classifier(1024,10).to(device)\n",
    "C3 = Classifier(2048,10).to(device)\n",
    "D1 = Discriminator(2048,2).to(device)\n",
    "D2 = Discriminator(1024,2).to(device)\n",
    "D3= Discriminator(2048,2).to(device)\n",
    "\n",
    "optimizer1 = torch.optim.AdamW(list(F1.parameters()) +list(C1.parameters()) +list(D1.parameters()))\n",
    "train(usps_loader,mnist_loader,F1,C1,D1,optimizer1,writer1,'./DANN_USPS-MNIST_1.pt','./USPS-MNIST_1.txt')\n",
    "\n",
    "optimizer2 = torch.optim.AdamW(list(F2.parameters()) +list(C2.parameters()) +list(D2.parameters()))\n",
    "train(usps_loader,mnist_loader,F2,C2,D2,optimizer2,writer2,'./DANN_USPS-MNIST_2.pt','./USPS-MNIST_2.txt')\n",
    "\n",
    "optimizer3 = torch.optim.AdamW(list(F3.parameters()) +list(C3.parameters()) +list(D3.parameters()))\n",
    "train(usps_loader,mnist_loader,F3,C3,D3,optimizer3,writer3,'./DANN_USPS-MNIST_3.pt','./USPS-MNIST_3.txt')\n",
    "\n",
    "\n",
    "writer1 = SummaryWriter('runs/DANN/MNIST-USPS/1')\n",
    "writer2 = SummaryWriter('runs/DANN/MNIST-USPS/2')\n",
    "writer3 = SummaryWriter('runs/DANN/MNIST-USPS/3')\n",
    "F1 = FeatureExtractor1().to(device)\n",
    "F2 = FeatureExtractor2().to(device)\n",
    "F3 = FeatureExtractor3().to(device)\n",
    "C1= Classifier(2048,10).to(device)\n",
    "C2 = Classifier(1024,10).to(device)\n",
    "C3 = Classifier(2048,10).to(device)\n",
    "D1 = Discriminator(2048,2).to(device)\n",
    "D2 = Discriminator(1024,2).to(device)\n",
    "D3= Discriminator(2048,2).to(device)\n",
    "\n",
    "optimizer1 = torch.optim.AdamW(list(F1.parameters()) +list(C1.parameters()) +list(D1.parameters()))\n",
    "train(mnist_loader,usps_loader,F1,C1,D1,optimizer1,writer1,'./DANN_mnist_USPS_1.pt','./MNIST-USPS_1.txt')\n",
    "\n",
    "optimizer2 = torch.optim.AdamW(list(F2.parameters()) +list(C2.parameters()) +list(D2.parameters()))\n",
    "train(mnist_loader,usps_loader,F2,C2,D2,optimizer2,writer2,'./DANN_mnist_USPS_2.pt','./MNIST-USPS_2.txt')\n",
    "\n",
    "optimizer3 = torch.optim.AdamW(list(F3.parameters()) +list(C3.parameters()) +list(D3.parameters()))\n",
    "train(mnist_loader,usps_loader,F3,C3,D3,optimizer3,writer3,'./DANN_mnist_USPS_3.pt','./MNIST-USPS_3.txt')\n",
    "\n",
    "\n",
    "writer1 = SummaryWriter('runs/DANN/Office/R_C/1')\n",
    "writer2 = SummaryWriter('runs/DANN/Office/R_C/2')\n",
    "writer3 = SummaryWriter('runs/DANN/Office/R_C/3')\n",
    "F1 = FeatureExtractor1().to(device)\n",
    "F2 = FeatureExtractor2().to(device)\n",
    "F3 = FeatureExtractor3().to(device)\n",
    "C1= Classifier(2048,65).to(device)\n",
    "C2 = Classifier(4096,65).to(device)\n",
    "C3 = Classifier(8192,65).to(device)\n",
    "D1 = Discriminator(2048,2).to(device)\n",
    "D2 = Discriminator(4096,2).to(device)\n",
    "D3= Discriminator(8192,2).to(device)\n",
    "\n",
    "\n",
    "optimizer1 = torch.optim.AdamW(list(F1.parameters()) +list(C1.parameters()) +list(D1.parameters()))\n",
    "train(realworld_loader,clipart_loader,F1,C1,D1,optimizer1,writer1,'./DANN_Office_RC_1.pt','./Real-Clip_1.txt')\n",
    "optimizer2 = torch.optim.AdamW(list(F2.parameters()) +list(C2.parameters()) +list(D2.parameters()))\n",
    "train(realworld_loader,clipart_loader,F2,C2,D2,optimizer2,writer2,'./DANN_Office_RC_2.pt','./Real-Clip_2.txt')\n",
    "optimizer3 = torch.optim.AdamW(list(F3.parameters()) +list(C3.parameters()) +list(D3.parameters()))\n",
    "train(realworld_loader,clipart_loader,F3,C3,D3,optimizer3,writer3,'./DANN_Office_RC_3.pt','./Real-Clip_3.txt')\n",
    "\n",
    "writer1 = SummaryWriter('runs/DANN/Office/C_R/1')\n",
    "writer2 = SummaryWriter('runs/DANN/Office/C_R/2')\n",
    "writer3 = SummaryWriter('runs/DANN/Office/C_R/3')\n",
    "F1 = FeatureExtractor1().to(device)\n",
    "F2 = FeatureExtractor2().to(device)\n",
    "F3 = FeatureExtractor3().to(device)\n",
    "C1= Classifier(2048,65).to(device)\n",
    "C2 = Classifier(4096,65).to(device)\n",
    "C3 = Classifier(8192,65).to(device)\n",
    "D1 = Discriminator(2048,2).to(device)\n",
    "D2 = Discriminator(4096,2).to(device)\n",
    "D3= Discriminator(8192,2).to(device)\n",
    "\n",
    "\n",
    "optimizer1 = torch.optim.AdamW(list(F1.parameters()) +list(C1.parameters()) +list(D1.parameters()))\n",
    "train(clipart_loader,realworld_loader,F1,C1,D1,optimizer1,writer1,'./DANN_Office_CR_1.pt','./Clip-Real_1.txt')\n",
    "\n",
    "optimizer2 = torch.optim.AdamW(list(F2.parameters()) +list(C2.parameters()) +list(D2.parameters()))\n",
    "train(clipart_loader,realworld_loader,F2,C2,D2,optimizer2,writer2,'./DANN_Office_CR_2.pt','./Clip-Real_2.txt')\n",
    "\n",
    "optimizer3 = torch.optim.AdamW(list(F3.parameters()) +list(C3.parameters()) +list(D3.parameters()))\n",
    "train(clipart_loader,realworld_loader,F3,C3,D3,optimizer3,writer3,'./DANN_Office_CR_3.pt','./Clip-Real_3.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ADDA</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters and supporing functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "d_learning_rate = 1e-4\n",
    "c_learning_rate = 1e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "lambda_gp = 10\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "def make_variable(tensor):\n",
    "    \"\"\"Convert Tensor to Variable.\"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "    \n",
    "    return Variable(tensor,requires_grad= False)\n",
    "\n",
    "def init_random_seed(manual_seed):\n",
    "    \"\"\"Init random seed.\"\"\"\n",
    "    seed = None\n",
    "    if manual_seed is None:\n",
    "        seed = random.randint(1, 10000)\n",
    "    else:\n",
    "        seed = manual_seed\n",
    "    print(\"use random seed: {}\".format(seed))\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def save_model(model,path):\n",
    "    torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" encoder model for ADDA.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        #self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.model.children())[:-1])\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.feature_extractor(x).squeeze(2).squeeze(2)\n",
    "        return x\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,in_features=2048,num_classes=10):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Linear(in_features, 512),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(128, num_classes)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "\n",
    "        return self.classifier(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_features=2048,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        return self.discriminator(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(crit, real, fake, epsilon):\n",
    "    '''\n",
    "    Return the gradient of the critic's scores with respect to mixes of real and fake images.\n",
    "    Parameters:\n",
    "        crit: the critic model\n",
    "        real: a batch of real images\n",
    "        fake: a batch of fake images\n",
    "        epsilon: a vector of the uniformly random proportions of real/fake per mixed image\n",
    "    Returns:\n",
    "        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
    "    '''\n",
    "    # Mix the images together\n",
    "\n",
    "    #print(f'Real size = {real.size()} Fake size = {fake.size()}')\n",
    "    mixed_images = real * epsilon + fake * (1 - epsilon)\n",
    "\n",
    "    # Calculate the critic's scores on the mixed images\n",
    "    mixed_scores = crit(mixed_images)\n",
    "    \n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        # Note: You need to take the gradient of outputs with respect to inputs.\n",
    "        # This documentation may be useful, but it should not be necessary:\n",
    "        # https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
    "        #### START CODE HERE ####\n",
    "        inputs=mixed_images,\n",
    "        outputs=mixed_scores,\n",
    "        #### END CODE HERE ####\n",
    "        # These other parameters have to do with the pytorch autograd engine works\n",
    "        grad_outputs=torch.ones_like(mixed_scores), \n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    # Flatten the gradients so that each row captures one image\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "\n",
    "    # Calculate the magnitude of every row\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    \n",
    "    # Penalize the mean squared distance of the gradient norms from 1\n",
    "    #### START CODE HERE ####\n",
    "    penalty = torch.mean((gradient_norm - 1)**2)\n",
    "    #### END CODE HERE ####\n",
    "    return penalty\n",
    "    \n",
    "\n",
    "def train_src(encoder, classifier, data_loader,writer,logfile,num_epochs = 20):\n",
    "    \"\"\"Train classifier for source domain.\"\"\"\n",
    "    ####################\n",
    "    # 1. setup network #\n",
    "    ####################\n",
    "\n",
    "    # set train state for Dropout and BN layers\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "\n",
    "    # setup criterion and optimizer\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(classifier.parameters()),\n",
    "        lr=c_learning_rate,\n",
    "        betas=(beta1, beta2))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    ####################\n",
    "    # 2. train network #\n",
    "    ####################\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, (images, labels) in enumerate(data_loader):\n",
    "\n",
    "            # make images and labels variable\n",
    "            \n",
    "            if images.size(1)==1:\n",
    "                images = torch.cat([images,images,images],dim=1)\n",
    "            \n",
    "            \n",
    "            images = make_variable(images)\n",
    "            labels = make_variable(labels.squeeze_())\n",
    "\n",
    "            # zero gradients for optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute loss for critic\n",
    "            preds = classifier(encoder(images))\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            # optimize source classifier\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('Loss',loss.item(),global_step)\n",
    "            global_step += 1\n",
    "\n",
    "            # print step info\n",
    "            if ((step + 1) % 20 == 0):\n",
    "                with open(logfile,'a') as f:\n",
    "                    print(f\"Epoch [{epoch + 1}/{num_epochs}] Step [{step + 1}/{len(data_loader)}] \\\n",
    "                       loss={loss.item()} \",file = f)\n",
    "               \n",
    "\n",
    "        # eval model on test set\n",
    "        if ((epoch + 1) % 20 == 0):\n",
    "            eval_src(encoder, classifier, data_loader)\n",
    "\n",
    "        # save model parameters\n",
    "        if ((epoch + 1) % 100 == 0):\n",
    "            save_model(encoder, f\"./ADDA_new/ADDA-source-encoder-{epoch + 1}.pt\")\n",
    "            save_model(\n",
    "                classifier, f\"./ADDA_new/ADDA-source-classifier-{epoch + 1}.pt\")\n",
    "\n",
    "    # # save final model\n",
    "    torch.save(encoder.state_dict(),\"./ADDA_new/ADDA-source-encoder-final.pt\")\n",
    "    torch.save(classifier.state_dict(), \"./ADDA_new/ADDA-source-classifier-final.pt\")\n",
    "\n",
    "    return encoder, classifier\n",
    "\n",
    "\n",
    "def eval_src(encoder, classifier, data_loader):\n",
    "    \"\"\"Evaluate classifier for source domain.\"\"\"\n",
    "    # set eval state for Dropout and BN layers\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    # init loss and accuracy\n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "\n",
    "    # set loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # evaluate network\n",
    "    for (images, labels) in data_loader:\n",
    "        \n",
    "        if images.size(1)==1:\n",
    "            images = torch.cat([images,images,images],dim=1)\n",
    "        \n",
    "        \n",
    "        images = make_variable(images)\n",
    "        labels = make_variable(labels)\n",
    "\n",
    "        preds = classifier(encoder(images))\n",
    "        loss += criterion(preds, labels).item()\n",
    "\n",
    "        pred_cls = preds.data.max(1)[1]\n",
    "        acc += pred_cls.eq(labels.data).cpu().sum()\n",
    "\n",
    "    loss = loss/len(data_loader)\n",
    "    acc = acc/len(data_loader.dataset)\n",
    "\n",
    "    print(f\"Avg Loss = {loss}, Avg Accuracy = {acc}\")\n",
    "\n",
    "def train_tgt(src_encoder, tgt_encoder, critic,classifier,\n",
    "              src_data_loader, tgt_data_loader,writer,logfile,num_epochs=100):\n",
    "    \"\"\"Train encoder for target domain.\"\"\"\n",
    "    ####################\n",
    "    # 1. setup network #\n",
    "    ####################\n",
    "\n",
    "    # set train state for Dropout and BN layers\n",
    "    tgt_encoder.train()\n",
    "    critic.train()\n",
    "\n",
    "    # setup criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_tgt = optim.Adam(tgt_encoder.parameters(),\n",
    "                               lr=c_learning_rate,\n",
    "                               betas=(beta1, beta2))\n",
    "    optimizer_critic = optim.Adam(critic.parameters(),\n",
    "                                  lr=d_learning_rate,\n",
    "                                  betas=(beta1,beta2))\n",
    "    len_data_loader = min(len(src_data_loader), len(tgt_data_loader))\n",
    "    best_acc = 0.0\n",
    "\n",
    "    ####################\n",
    "    # 2. train network #\n",
    "    ####################\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # zip source and target data pair\n",
    "        data_zip = enumerate(zip(src_data_loader, tgt_data_loader))\n",
    "        for step, ((images_src, _), (images_tgt, _)) in data_zip:\n",
    "            critic.train()\n",
    "            tgt_encoder.train()\n",
    "            ###########################\n",
    "            # 2.1 train discriminator #\n",
    "            ###########################\n",
    "            \n",
    "            if images_src.size(1)==1:\n",
    "                images_src = torch.cat([images_src,images_src,images_src],dim=1)\n",
    "            if images_tgt.size(1)==1:\n",
    "                images_tgt = torch.cat([images_tgt,images_tgt,images_tgt],dim=1)\n",
    "            \n",
    "            # make images variable\n",
    "            images_src = make_variable(images_src)\n",
    "            images_tgt = make_variable(images_tgt)\n",
    "\n",
    "            # zero gradients for optimizer\n",
    "            optimizer_critic.zero_grad()\n",
    "\n",
    "            # extract and concat features\n",
    "            feat_src = src_encoder(images_src)\n",
    "            feat_tgt = tgt_encoder(images_tgt)\n",
    "            \n",
    "            real_pred = critic(feat_src)\n",
    "            fake_pred = critic(feat_tgt)\n",
    "\n",
    "            epsilon = torch.rand(len(feat_tgt), 1, 1, 1, device=device, requires_grad=True)\n",
    "            gp = gradient_penalty(critic, feat_src, feat_tgt, epsilon)\n",
    "        \n",
    "            # Adversarial loss\n",
    "            loss_critic = -torch.mean(real_pred) + torch.mean(fake_pred) + lambda_gp * gp\n",
    "\n",
    "            # compute loss for critic\n",
    "            loss_critic.backward()\n",
    "\n",
    "            # optimize critic\n",
    "            optimizer_critic.step()\n",
    "\n",
    "\n",
    "            ############################\n",
    "            # 2.2 train target encoder #\n",
    "            ############################\n",
    "\n",
    "            # zero gradients for optimizer\n",
    "            optimizer_critic.zero_grad()\n",
    "            optimizer_tgt.zero_grad()\n",
    "\n",
    "            # extract and target features\n",
    "            feat_tgt = tgt_encoder(images_tgt)\n",
    "\n",
    "            # predict on discriminator\n",
    "            pred_tgt = critic(feat_tgt)\n",
    "\n",
    "            # prepare fake labels\n",
    "            label_tgt = make_variable(torch.ones(feat_tgt.size(0)).long())\n",
    "\n",
    "            # compute loss for target encoder\n",
    "            loss_tgt = criterion(pred_tgt, label_tgt)\n",
    "            loss_tgt.backward()\n",
    "\n",
    "            # optimize target encoder\n",
    "            optimizer_tgt.step()\n",
    "            writer.add_scalar('Loss Critic',loss_critic.item(),global_step)\n",
    "            writer.add_scalar('Loss Tgt',loss_tgt.item(),global_step)\n",
    "   \n",
    "            global_step += 1\n",
    "            #######################\n",
    "            # 2.3 print step info #\n",
    "            #######################\n",
    "            if ((step + 1) % 20 == 0):\n",
    "\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}] Step [{step + 1}/{len_data_loader}]:\\\n",
    "                    d_loss={loss_critic.item()} g_loss={loss_tgt.item()}\")\n",
    "            \n",
    "        with torch.no_grad(): \n",
    "            loss = 0.0\n",
    "            tgt_acc = 0.0\n",
    "            tgt_encoder.eval()\n",
    "            classifier.eval()\n",
    "            for (images, labels) in tgt_data_loader:\n",
    "                if images.size(1)==1:\n",
    "                    images = torch.cat([images,images,images],dim=1)\n",
    "                images = make_variable(images)\n",
    "                labels = make_variable(labels).squeeze_()\n",
    "\n",
    "                preds = classifier(tgt_encoder(images))\n",
    "                loss += criterion(preds, labels).item()\n",
    "\n",
    "                pred_cls = preds.data.max(1)[1]\n",
    "                tgt_acc += pred_cls.eq(labels.data).cpu().sum()\n",
    "            loss /= len(tgt_data_loader)\n",
    "            tgt_acc /= len(tgt_data_loader.dataset)\n",
    "        with open(logfile,'a') as f:\n",
    "            print(f\"Epoch : {epoch} tgt_acc = {tgt_acc}\",file = f)\n",
    "\n",
    "        #############################\n",
    "        # 2.4 save model parameters #\n",
    "        #############################\n",
    "        if best_acc < tgt_acc:\n",
    "            best_acc = tgt_acc\n",
    "            torch.save(critic.state_dict(),\"./ADDA_new/ADDA-critic-final.pt\")\n",
    "            torch.save(tgt_encoder.state_dict(),\"./ADDA_new/ADDA-target-encoder-final.pt\")\n",
    "            \n",
    "    return tgt_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "src_data_loader = mnist_dataloader()\n",
    "\n",
    "tgt_data_loader = usps_dataloader()\n",
    "\n",
    "# load models\n",
    "src_encoder = Encoder().to(device)\n",
    "\n",
    "src_classifier = Classifier(num_classes=65).to(device)\n",
    "\n",
    "tgt_encoder = Encoder().to(device)\n",
    "\n",
    "critic = Discriminator().to(device)\n",
    "\n",
    "writer_src = SummaryWriter('runs/ADDA/MNIST-USPS/train_src')\n",
    "writer_tgt = SummaryWriter('runs/ADDA/MNIST-USPS//train_tgt')\n",
    "\n",
    "train_tgt_logs = './ADDA_new/MNIST-USPS/train_tgt_logs.txt'\n",
    "train_src_logs = './ADDA_new/MNIST-USPS/train_src_logs.txt'\n",
    "\n",
    "\n",
    "if os.path.exists(\"./ADDA_new/MNIST-USPS//ADDA-source-encoder-final.pt\"):\n",
    "    src_encoder.load_state_dict(torch.load(\"./ADDA_new/MNIST-USPS/ADDA-source-encoder-final.pt\"))\n",
    "    src_classifier.load_state_dict(torch.load(\"./ADDA_new/MNIST-USPS/ADDA-source-classifier-final.pt\"))\n",
    "else:\n",
    "    src_encoder, src_classifier = train_src(src_encoder, src_classifier, src_data_loader,writer_src,train_src_logs)\n",
    "\n",
    "# eval source model\n",
    "eval_src(src_encoder, src_classifier, src_data_loader)\n",
    "\n",
    "tgt_encoder.load_state_dict(src_encoder.state_dict())\n",
    "\n",
    "if os.path.exists(\"./ADDA_new/MNIST-USPS/ADDA-target-encoder-final.pt\"):\n",
    "    tgt_encoder.load_state_dict(torch.load(\"./ADDA_new/MNIST-USPS/ADDA-target-encoder-final.pt\"))\n",
    "else:  \n",
    "\n",
    "    tgt_encoder = train_tgt(src_encoder, tgt_encoder, critic,src_classifier, src_data_loader, tgt_data_loader,\\\n",
    "                            writer_tgt,train_tgt_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CycleGAN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('conv') != -1:\n",
    "        nn.init.normal_(w.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('bn') != -1:\n",
    "        nn.init.normal_(w.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(w.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(c_in, c_out, k_size=4, stride=2, pad=1, use_bn=True, transpose=False):\n",
    "    module = []\n",
    "    if transpose:\n",
    "        module.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, output_padding=pad, bias=not use_bn))\n",
    "    else:\n",
    "        module.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
    "    if use_bn:\n",
    "        module.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*module)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = conv_block(channels, channels, k_size=3, stride=1, pad=1, use_bn=True)\n",
    "        self.conv2 = conv_block(channels, channels, k_size=3, stride=1, pad=1, use_bn=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x + self.conv2(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = conv_block(params['nc'], params['ndf'], use_bn=False)\n",
    "        self.conv2 = conv_block(params['ndf'], params['ndf'] * 2)\n",
    "        self.conv3 = conv_block(params['ndf'] * 2, params['ndf'] * 4)\n",
    "        self.conv4 = conv_block(params['ndf'] * 4, 1, k_size=3, stride=1, pad=1, use_bn=False)\n",
    "\n",
    "        # Initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b_size = x.shape[0]\n",
    "        x = x.view(b_size,1,32,32)\n",
    "        alpha = 0.2\n",
    "        x = F.leaky_relu(self.conv1(x), alpha)\n",
    "        x = F.leaky_relu(self.conv2(x), alpha)\n",
    "        x = F.leaky_relu(self.conv3(x), alpha)\n",
    "        x = self.conv4(x)\n",
    "        x = x.reshape([x.shape[0], -1]).mean(1)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = conv_block(params['nc'], params['ngf'], k_size=5, stride=1, pad=2, use_bn=True)\n",
    "        self.conv2 = conv_block(params['ngf'], params['ngf'] * 2, k_size=3, stride=2, pad=1, use_bn=True)\n",
    "        self.conv3 = conv_block(params['ngf'] * 2, params['ngf'] * 4, k_size=3, stride=2, pad=1, use_bn=True)\n",
    "        self.res4 = ResBlock(params['ngf'] * 4)\n",
    "        self.tconv5 = conv_block(params['ngf'] * 4, params['ngf'] * 2, k_size=3, stride=2, pad=1, use_bn=True, transpose=True)\n",
    "        self.tconv6 = conv_block(params['ngf'] * 2, params['ngf'], k_size=3, stride=2, pad=1, use_bn=True, transpose=True)\n",
    "        self.conv7 = conv_block(params['ngf'], params['nc'], k_size=5, stride=1, pad=2, use_bn=False)\n",
    "\n",
    "        # Initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b_size = x.shape[0]\n",
    "        x = x.view(b_size,1,32,32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.res4(x))\n",
    "        x = F.relu(self.tconv5(x))\n",
    "        x = F.relu(self.tconv6(x))\n",
    "        x = torch.tanh(self.conv7(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 369\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "params = {\n",
    "    \"bsize\" : 256,# Batch size during training.\n",
    "    'imsize' : 32,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 1,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 64,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 64, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 50,# Number of training epochs.\n",
    "    'lr' : 0.0002,# Learning rate for optimizers\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'save_epoch' : 2,# Save step.\n",
    "    'lambda_cyc':10.0,\n",
    "    'lambda_id':5.0}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "MNISTData = mnistDataset('mnist')\n",
    "USPSData = uspsDataset('usps/usps.h5')\n",
    "mnistLoader = DataLoader(MNISTData, params['bsize'], shuffle=True)\n",
    "uspsLoader = DataLoader(USPSData, params['bsize'], shuffle=True)\n",
    "iters_per_epoch = min(len(mnistLoader), len(uspsLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG_AB = Generator(params).to(device)\n",
    "netG_AB.apply(weights_init)\n",
    "netG_BA = Generator(params).to(device)\n",
    "netG_BA.apply(weights_init)\n",
    "netD_A = Discriminator(params).to(device)\n",
    "netD_A.apply(weights_init)\n",
    "netD_B = Discriminator(params).to(device)\n",
    "netD_B.apply(weights_init)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "fixed_noise = torch.randn(64, params['nz'], 1, 1, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(list(netD_A.parameters())+list(netD_B.parameters()), lr=params['lr'], betas=(params['beta1'], 0.999), weight_decay=2e-5)\n",
    "optimizerG = optim.Adam(list(netG_AB.parameters())+list(netG_BA.parameters()), lr=params['lr'], betas=(params['beta1'], 0.999), weight_decay=2e-5)\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "prev_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_fixed = iter(mnistLoader).next()[0]\n",
    "b_fixed = iter(uspsLoader).next()[0]\n",
    "\n",
    "a_fixed = a_fixed.cuda()\n",
    "b_fixed = b_fixed.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "print(\"-\"*25)\n",
    "recon_loss, gan_loss, dis_loss = [],[],[]\n",
    "for epoch in range(params['nepochs']):\n",
    "    netG_AB.train()\n",
    "    netG_BA.train()\n",
    "    netD_A.train()\n",
    "    netD_B.train()\n",
    "    for i, (a_data, b_data) in enumerate(zip(mnistLoader, uspsLoader)):\n",
    "        a_real, _ = a_data\n",
    "        b_real, _ = b_data\n",
    "\n",
    "        a_real, b_real = a_real.float().cuda(), b_real.float().cuda()\n",
    "\n",
    "        # Fake Images\n",
    "        b_fake = netG_AB(a_real)\n",
    "        a_fake = netG_BA(b_real)\n",
    "\n",
    "        # Training discriminator\n",
    "        a_real_out = netD_A(a_real)\n",
    "        a_fake_out = netD_A(a_fake.detach())\n",
    "        a_d_loss = (torch.mean((a_real_out - 1) ** 2) + torch.mean(a_fake_out ** 2)) / 2\n",
    "\n",
    "        b_real_out = netD_B(b_real)\n",
    "        b_fake_out = netD_B(b_fake.detach())\n",
    "        b_d_loss = (torch.mean((b_real_out - 1) ** 2) + torch.mean(b_fake_out ** 2)) / 2\n",
    "\n",
    "        optimizerD.zero_grad()\n",
    "        d_loss = a_d_loss + b_d_loss\n",
    "        d_loss.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Training Generator\n",
    "        a_fake_out = netD_A(a_fake)\n",
    "        b_fake_out = netD_B(b_fake)\n",
    "\n",
    "        a_g_loss = torch.mean((a_fake_out - 1) ** 2)\n",
    "        b_g_loss = torch.mean((b_fake_out - 1) ** 2)\n",
    "        g_gan_loss = a_g_loss + b_g_loss\n",
    "\n",
    "        a_g_ctnt_loss = (a_real - netG_BA(b_fake)).abs().mean()\n",
    "        b_g_ctnt_loss = (b_real - netG_AB(a_fake)).abs().mean()\n",
    "        g_ctnt_loss = a_g_ctnt_loss + b_g_ctnt_loss\n",
    "        recon_loss.append([g_ctnt_loss.item()])\n",
    "        gan_loss.append([g_gan_loss.item()])\n",
    "        dis_loss.append([d_loss.item()])\n",
    "        optimizerG.zero_grad()\n",
    "        g_loss = g_gan_loss + g_ctnt_loss\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(params['nepochs'])\n",
    "                    + \" it: \" + str(i) + \"/\" + str(iters_per_epoch)\n",
    "                    + \"\\ta_d_loss:\" + str(round(a_d_loss.item(), 4))\n",
    "                    + \"\\ta_g_loss:\" + str(round(a_g_loss.item(), 4))\n",
    "                    + \"\\ta_g_ctnt_loss:\" + str(round(a_g_ctnt_loss.item(), 4))\n",
    "                    + \"\\tb_d_loss:\" + str(round(b_d_loss.item(), 4))\n",
    "                    + \"\\tb_g_loss:\" + str(round(b_g_loss.item(), 4))\n",
    "                    + \"\\tb_g_ctnt_loss:\" + str(round(b_g_ctnt_loss.item(), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'generator_AB' : netG_AB.state_dict(),\n",
    "            'discriminator_A' : netD_A.state_dict(),\n",
    "            'generator_BA' : netG_BA.state_dict(),\n",
    "            'discriminator_B' : netD_B.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'final_model.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(recon_loss)), np.array(recon_loss),color='r', label='Reconstruction Loss')\n",
    "plt.plot(np.arange(len(recon_loss)), np.array(gan_loss), color='g', label='Generator Loss')\n",
    "plt.plot(np.arange(len(recon_loss)), np.array(dis_loss), color='b', label='Discriminator Loss')\n",
    "  \n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Iterations(CycleGAN)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(params['imsize'], 10, True).to(device)\n",
    "model.load_state_dict(torch.load('output/trained_on_mnist_resnet.pth')['model'])\n",
    "eval_cycleGAN(model, netG_AB, MNISTData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(params['imsize'], 10, True).to(device)\n",
    "model.load_state_dict(torch.load('output/trained_on_usps_resnet.pth')['model'])\n",
    "eval_cycleGAN(model, netG_BA, USPSData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.randint(0, len(USPSData), size=50)\n",
    "generated_img = netG_BA(torch.tensor(USPSData[r][0]).float().cuda().abs()).cpu().detach().numpy().reshape(50, 32, 32)\n",
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, i in zip(grid, range(50)):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(USPSData[r[i]][0], cmap='gray')\n",
    "    ax.imshow(np.clip(generated_img[i], 0, 1), cmap='gray')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16f5b46f222e2a3e8d4adbf7141cae37b71ed37616e60735fa5d1164a1bc3ada"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
