{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "import glob\n",
    "from pickletools import optimize\n",
    "import time\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from skimage import io, transform\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VAE</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global Variables\n",
    "batch_size  = 100\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Supporting functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "## Get Variance of a dataloader\n",
    "def get_variance(loader):\n",
    "    num_batches,total,squared_sum = 0,0,0\n",
    "    \n",
    "    for data in loader:\n",
    "        total += torch.mean(data,dim=[0,1,2,3])\n",
    "        squared_sum += torch.mean(data**2,dim=[0,1,2,3])\n",
    "        num_batches += 1\n",
    "    variance = squared_sum/num_batches - (total/num_batches)**2\n",
    "    return variance\n",
    "\n",
    "def get_variance_batch(batch):\n",
    "    \n",
    "    total = torch.mean(data,dim=[0,1,2,3])\n",
    "    squared_sum = torch.mean(data**2,dim=[0,1,2,3])\n",
    "    variance = squared_sum - (total)**2\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Loaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dsprites_Dataset(Dataset):\n",
    "    def __init__(self, data , transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.data[idx]\n",
    "\n",
    "class CelebA_Dataset(Dataset):\n",
    "    def __init__(self, data_list , img_dir, transform=None, target_transform=None):\n",
    "        self.img_titles = data_list\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_titles [idx])\n",
    "        image = PIL.Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Vanilla VAE class implementation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 latent_dim,\n",
    "                 hidden_dims,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.in_channels = in_channels\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1]*4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= self.in_channels,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input) :\n",
    "        \n",
    "        result = self.encoder(input)\n",
    "        \n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        \n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z):\n",
    "        \n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu, logvar) :\n",
    "        \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        \n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z),input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,*args):\n",
    "        recons = args[0]\n",
    "        input_ = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        variance = args[4]\n",
    "        \n",
    "        recons_loss = F.mse_loss(recons, input_)/(2*variance)\n",
    "        kl_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kl_loss\n",
    "        return [loss, recons_loss.detach(), kl_loss.detach()]\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) :\n",
    "       \n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Early Stopping</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train Function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(in_channels,latent_dim,train_dataloader,val_dataloader,checkpoint_path,train_variance,val_variance):\n",
    "    model = VanillaVAE(in_channels=in_channels,latent_dim=latent_dim,hidden_dims=None).to(device)\n",
    "    optimizer =  torch.optim.Adam(model.parameters(),lr = 1e-3)\n",
    "    epochs = 10\n",
    "    train_loss,train_kl,train_recons = [],[],[]\n",
    "    earlystopping = EarlyStopping(patience=5, verbose=False, delta=0, path=checkpoint_path)\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss,avg_kl,avg_recon = 0,0,0\n",
    "        start = time.time()\n",
    "        for idx,data in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            data = data.to(device)\n",
    "            output,input_,mu,log_var = model(data)\n",
    "            loss,recons_loss,kl_loss = model.loss_function(output,input_,mu,log_var,train_variance)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            avg_loss += loss.item()\n",
    "            avg_kl += kl_loss.item()\n",
    "            avg_recon += recons_loss.item()\n",
    "        train_loss.append(avg_loss/idx),train_kl.append(avg_kl/idx),train_recons.append(avg_recon/idx)\n",
    "        end = time.time()\n",
    "        print(f' Time taken for epoch = {epoch} = {end-start}, Loss = {avg_loss/idx}, KL Loss = {avg_kl/idx}, recons_loss = {avg_recon/idx}')\n",
    "        with torch.no_grad():\n",
    "            for idx,data in enumerate(val_dataloader):\n",
    "                model.eval()\n",
    "                data = data.to(device)\n",
    "                output,input_,mu,log_var = model(data)\n",
    "                val_loss,val_recons_loss,val_kl_loss = model.loss_function(output,input_,mu,log_var,val_variance)\n",
    "            earlystopping(val_loss,model)\n",
    "        if epoch ==  epochs-1:\n",
    "            torch.save(model.state_dict(),checkpoint_path)\n",
    "\n",
    "    plt.figure(0)\n",
    "    plt.plot(np.arange(len(train_loss)),train_loss,label = 'Train Loss')\n",
    "    plt.plot(np.arange(len(train_kl)),train_kl,label = 'KL Loss')\n",
    "    plt.plot(np.arange(len(train_recons)),train_loss,label = 'Train Recons')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Intialization, training and visualization for dsprite dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz')\n",
    "\n",
    "imgs = torch.tensor(data['imgs'].reshape(-1,1,64,64),dtype = torch.float32)\n",
    "imgs = imgs[torch.randperm(imgs.size()[0])]\n",
    "train_len = int(0.9*imgs.size()[0])   ## 90-10 train val split\n",
    "train_imgs,val_imgs = imgs[0:train_len],imgs[train_len:]\n",
    "\n",
    "train_dataset = Dsprites_Dataset(train_imgs)\n",
    "val_dataset = Dsprites_Dataset(val_imgs)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size = batch_size,shuffle=True)\n",
    "\n",
    "train_variance = get_variance(train_dataloader).item()\n",
    "val_variance = get_variance(val_dataloader).item()\n",
    "\n",
    "\n",
    "train(1,128,train_dataloader,val_dataloader,'/home/hiren/Apoorv Pandey/ADRL/Ass1/VanillaVAE_dsprites.pt',train_variance,val_variance)\n",
    "\n",
    "\n",
    "## Visualize Generated Images of Dsprites\n",
    "model = VanillaVAE(in_channels=1,latent_dim=128,hidden_dims = None).to(device)\n",
    "model.load_state_dict(torch.load('/home/hiren/Apoorv Pandey/ADRL/Ass1/VanillaVAE_dsprites.pt'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "val_originals = next(iter(val_dataloader))\n",
    "val_originals = val_originals.to(device)\n",
    "val_recons,_,_,_ = model(val_originals)\n",
    "show(make_grid(val_originals.cpu(),nrow=10))\n",
    "show(make_grid(val_recons.cpu(),nrow=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Intialization, training and visualization for CelebA dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/hiren/Apoorv Pandey/ADRL/Ass1/list_eval_partition.csv')\n",
    "train_data,val_data,test_data = data[data['partition']==0]['image_id'].to_list(),data[data['partition']==1]['image_id'].to_list(),\\\n",
    "                                data[data['partition']==2]['image_id'].to_list()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Resize((64,64))])\n",
    "train_dataset = CelebA_Dataset(train_data,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba',transform)\n",
    "val_dataset = CelebA_Dataset(val_data,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba',transform)\n",
    "test_dataset = CelebA_Dataset(test_data,'/home/hiren/Apoorv Pandey/ADRL/Ass1/img_align_celeba/img_align_celeba')\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "train_variance = get_variance(train_loader)\n",
    "val_variance = get_variance(val_loader)\n",
    "train(3,128,train_loader,val_loader,'/home/hiren/Apoorv Pandey/ADRL/Ass1/VanillaVAE_Celeb.pt',train_variance,val_variance) ## Train Celeb A dataset\n",
    "\n",
    "\n",
    "## Visualize Generated Images of Celeb A\n",
    "model = VanillaVAE(in_channels=3,latent_dim=128,hidden_dims=None).to(device)\n",
    "model.load_state_dict(torch.load('/home/hiren/Apoorv Pandey/ADRL/Ass1/VanillaVAE_Celeb.pt'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "val_originals = next(iter(val_loader))\n",
    "val_originals = val_originals.to(device)\n",
    "val_recons,_,_,_ = model(val_originals)\n",
    "show(make_grid(val_originals.cpu(),nrow=10))\n",
    "show(make_grid(val_recons.cpu(),nrow=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Caluclating Marginal Likelihood Of data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size = 10,shuffle=False)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = VanillaVAE(in_channels=3,latent_dim=128,hidden_dims=None).to(device)\n",
    "    model.load_state_dict(torch.load('/home/hiren/Apoorv Pandey/ADRL/Ass1/VanillaVAE_Celeb.pt'))\n",
    "\n",
    "    marginal_likelihood = 0\n",
    "    num_samples = 0\n",
    "    L = 5 \n",
    "    sampled_z_list = []\n",
    "    data = next(iter(train_loader))\n",
    "    data = data.to(device)\n",
    "    outputs = model(data)[0]\n",
    "    train_var = get_variance_batch(data).item()\n",
    "    print(train_var)\n",
    "    batch_size = data.size()[0]\n",
    "    mu , logvar = model.encode(data)\n",
    "    sampled_z = torch.zeros(L,batch_size,128)\n",
    "    for i in range(L):\n",
    "        sampled_z[i] = model.reparameterize(mu,logvar)\n",
    "\n",
    "    sampled_z = sampled_z.view(batch_size*L,-1)\n",
    "    sampled_z_np = sampled_z.numpy()\n",
    "    sampled_z_pca = PCA(n_components=4).fit_transform(sampled_z_np)\n",
    "    posterior_density = GaussianMixture(n_components=32, random_state=0).fit(sampled_z_pca)\n",
    "    \n",
    "    sampled_z_new = torch.zeros(L,batch_size,128)\n",
    "    sampled_z_new_list = []\n",
    "    for i in range(L):\n",
    "        sampled_z_new[i] = model.reparameterize(mu,logvar)\n",
    "\n",
    "    sampled_z_new = sampled_z_new.reshape(batch_size*L,-1)\n",
    "    sampled_z_new_list.append(sampled_z_new.cpu())\n",
    "    sampled_z_new = torch.cat(sampled_z_new_list,dim=0)\n",
    "    sampled_z_new_np = sampled_z_new.numpy()\n",
    "    sampled_z_new_pca = PCA(n_components=4).fit_transform(sampled_z_new_np)\n",
    "    \n",
    "    outputs = model.decode(sampled_z_new.to(device))\n",
    "    print(outputs.size())\n",
    "    log_like = posterior_density.score_samples(sampled_z_new_pca)  ## Fitted density likelihood\n",
    "    likelihood = np.exp(log_like)\n",
    "    \n",
    "    data_np = data.cpu().numpy().reshape(batch_size,-1)\n",
    "    outputs_np = outputs.cpu().numpy().reshape(batch_size*L,-1)\n",
    "    overall_likelihood = 0 \n",
    "    for i in tqdm(range(0,batch_size*L,L)):\n",
    "        sample_likelihood = 0\n",
    "        \n",
    "        for j in range(L):\n",
    "            current_sample = sampled_z_new_pca[i+j]\n",
    "            \n",
    "            prior = multivariate_normal.pdf(current_sample, mean=np.zeros_like(current_sample), cov = np.identity(current_sample.shape[0]))\n",
    "            decoder_prob = multivariate_normal.pdf(outputs_np[i+j], mean=data_np[i//L], cov = train_var*np.identity(data_np[i//L].shape[0]))\n",
    "            sample_likelihood += likelihood[i+j]/((prior*decoder_prob))\n",
    "            print(f'Q score = { likelihood[i+j]}, prior = {prior}, decoder prob = {decoder_prob}')\n",
    "        sample_likelihood = L/(sample_likelihood)\n",
    "        print(f'Likelihood for {i} sample = {sample_likelihood}')\n",
    "        overall_likelihood += sample_likelihood\n",
    "    overall_likelihood = overall_likelihood/batch_size\n",
    "    print(f'Marginal Likelihood = {overall_likelihood}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VQ-VAE</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataloaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, data_path_list, transform=None, target_transform=None):\n",
    "        self.img_titles = data_path_list\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = PIL.Image.open(self.img_titles[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "class LatentDataset(Dataset):\n",
    "    def __init__(self, data , transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VQVAE architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.codebook_dim = num_embeddings\n",
    "        \n",
    "        self.codebook = nn.Embedding(self.codebook_dim, self.embedding_dim)\n",
    "        self.codebook.weight.data.uniform_(-1/self.codebook_dim, 1/self.codebook_dim)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "       \n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "       \n",
    "        flat_input = inputs.view(-1, self.embedding_dim)\n",
    "        \n",
    "        \n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.codebook.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self.codebook.weight.t()))\n",
    "            \n",
    "       \n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.codebook_dim, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        \n",
    "        quantized = torch.matmul(encodings, self.codebook.weight).view(input_shape)\n",
    "        \n",
    "        \n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()  ## Passing encoder gradients directly to decoder\n",
    "        \n",
    "       \n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Encoder</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens//2,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=num_hiddens//2,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "        self.conv_3 = nn.Conv2d(in_channels=num_hiddens,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1, padding=1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv_1(inputs)\n",
    "        #print(f\"encoder image shape afte 1st layer = {x.size()}\")\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = F.relu(x)\n",
    "        #print(f\"encoder image shape afte 2nd layer = {x.size()}\")\n",
    "        x = self.conv_3(x)\n",
    "        #print(f\"encoder image shape afte 3rd layer = {x.size()}\")\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decoder</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens,final_out_channels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.out_channels = final_out_channels\n",
    "        self.conv_1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3, \n",
    "                                 stride=1, padding=1)\n",
    "        \n",
    "        self.conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, \n",
    "                                                out_channels=num_hiddens//2,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "        self.conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2, \n",
    "                                                out_channels=self.out_channels,\n",
    "                                                kernel_size=4, \n",
    "                                                stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = F.relu(x)\n",
    "        #print(f\"decoder image shape afte 1st layer = {x.size()}\")\n",
    "        x = self.conv_trans_1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(f\"decoder image shape afte 2nd layer = {x.size()}\")\n",
    "        x = self.conv_trans_2(x)\n",
    "        #print(f\"decoder image shape afte 3rd layer = {x.size()}\")\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_hiddens, \n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(3, num_hiddens)\n",
    "        self.pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, \n",
    "                                      out_channels=embedding_dim,\n",
    "                                      kernel_size=1, \n",
    "                                      stride=1)\n",
    "        \n",
    "        self.vq_vae = VQVAE(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self.decoder = Decoder(embedding_dim,\n",
    "                                num_hiddens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        z = self.pre_vq_conv(encoded)\n",
    "        loss, quantized = self.vq_vae(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, quantized,z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VanillaVAE_Conv</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE_Conv(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 latent_dim,num_hiddens,out_channels,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE_Conv, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.encoder = Encoder(in_channels,num_hiddens)\n",
    "        self.decoder = Decoder(2*in_channels,num_hiddens,out_channels)\n",
    "        self.fc_mu = nn.Linear(2048,latent_dim)\n",
    "        self.fc_var = nn.Linear(2048,latent_dim)\n",
    "        self.decoder_input = nn.Linear(latent_dim, 2048)\n",
    "    def encode(self, input) :\n",
    "        \n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1).to(device)\n",
    "        \n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z):\n",
    "        \n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 128 , 4, 4)\n",
    "        result = self.decoder(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu, logvar) :\n",
    "        \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        \n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z),input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,*args):\n",
    "        recons = args[0]\n",
    "        input_ = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        variance = args[4]\n",
    "        \n",
    "        recons_loss = F.mse_loss(recons, input_)\n",
    "        kl_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss+kl_loss\n",
    "        return [loss, recons_loss.detach(), kl_loss.detach()]\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) :\n",
    "       \n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VanillaVAE_Linear</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE_Linear(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_dims,latent_dim) -> None:\n",
    "        super(VanillaVAE_Linear, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.append(nn.Sequential(nn.Linear(hidden_dims[i],hidden_dims[i+1]),nn.ReLU()))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        hidden_dims.reverse()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.append(nn.Sequential(nn.Linear(hidden_dims[i],hidden_dims[i+1]),nn.ReLU()))\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        hidden_dims.reverse()\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1],latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1],latent_dim)\n",
    "        self.decoder_input = nn.Linear(latent_dim,hidden_dims[-1])\n",
    "    def encode(self, input) :\n",
    "        #print(f'encoder input size = {input.size()}')\n",
    "        result = self.encoder(input)\n",
    "        #print(f'encoder output size = {result.size()}')\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "        #print(f'mu size = {mu.size()}')\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z):\n",
    "        \n",
    "        result = F.relu(self.decoder_input(z))\n",
    "        #print(f'decoder input size = {result.size()}')\n",
    "        result = self.decoder(result)\n",
    "        #print(f'decoder output size = {result.size()}')\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu, logvar) :\n",
    "        \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        \n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z),input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,*args):\n",
    "        recons = args[0]\n",
    "        input_ = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        variance = args[4]\n",
    "        \n",
    "        recons_loss = F.mse_loss(recons, input_)\n",
    "        kl_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kl_loss\n",
    "        return [loss, recons_loss.detach(), kl_loss.detach()]\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) :\n",
    "       \n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train Function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAE(model,in_channels,latent_dim,num_hiddens,train_dataloader,train_variance,path):\n",
    "    \n",
    "    optimizer =  torch.optim.AdamW(model.parameters(),lr = 1e-3)\n",
    "    epochs = 10\n",
    "    train_loss,train_kl,train_recons = [],[],[]\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss,avg_kl,avg_recon = 0,0,0\n",
    "        start = time.time()\n",
    "        for idx,data in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            data = data.to(device)\n",
    "            output,input_,mu,log_var = model(data)\n",
    "            loss,recons_loss,kl_loss = model.loss_function(output,data,mu,log_var,train_variance)\n",
    "            #loss = Variable(loss, requires_grad = True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            avg_loss += loss.item()\n",
    "            avg_kl += kl_loss.item()\n",
    "            avg_recon += recons_loss.item()\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "        train_loss.append(avg_loss/(idx+1)),train_kl.append(avg_kl/(idx+1)),train_recons.append(avg_recon/(idx+1))\n",
    "        print(f' loss = {avg_loss/(idx+1)}')\n",
    "    torch.save(model.state_dict(),path)\n",
    "    plt.figure(0)\n",
    "    plt.plot(np.arange(len(train_loss)),train_loss,label = 'Train Loss')\n",
    "    plt.plot(np.arange(len(train_kl)),train_kl,label = 'KL Loss')\n",
    "    plt.plot(np.arange(len(train_recons)),train_loss,label = 'Train Recons')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Intialization, Training and visualization for on tiny imagenet</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = glob.glob('/home/hiren/Apoorv Pandey/ADRL/Ass1/tiny-imagenet-200/train/*/images/*.JPEG')\n",
    "val_images_path = glob.glob('/home/hiren/Apoorv Pandey/ADRL/Ass1/tiny-imagenet-200/val/images/*.JPEG')\n",
    "test_images_path = glob.glob('/home/hiren/Apoorv Pandey/ADRL/Ass1/tiny-imagenet-200/test/images/*.JPEG')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = TinyImageNetDataset(train_images_path,transform)\n",
    "val_dataset = TinyImageNetDataset(val_images_path,transform)\n",
    "test_dataset = TinyImageNetDataset(test_images_path,transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=100,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=100,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=100,shuffle=True)\n",
    "\n",
    "train_variance = get_variance(train_loader)\n",
    "val_variance =  get_variance(val_loader)\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "\n",
    "num_hiddens = 128  ## channel width after passing through encoder\n",
    "\n",
    "embedding_dim = 64\n",
    "num_embeddings = 128  ## codebook dimension\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = Model(num_hiddens,num_embeddings, embedding_dim, commitment_cost).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_res_recon_error = []\n",
    "for i in range(num_epochs):\n",
    "    avg_recon_error,avg_perplexity = 0,0\n",
    "    steps = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        vq_loss, data_recon, quantized,encoded = model(data)\n",
    "        recon_error = F.mse_loss(data_recon, data)/(2*train_variance)\n",
    "        loss = recon_error + vq_loss\n",
    "        loss.backward()\n",
    "        avg_recon_error += recon_error.item()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "    train_res_recon_error.append(avg_recon_error/steps)\n",
    "\n",
    "    print(f' iterations {i+1}')\n",
    "    print(f'recon_error: %.3f {avg_recon_error/steps}')\n",
    "    print()\n",
    "torch.save(model.state_dict(),f'/home/hiren/Apoorv Pandey/ADRL/Ass1/VQ_VAE_TinyImageNet_{num_embeddings}.pt')\n",
    "\n",
    "## Visualize images\n",
    "\n",
    "model = Model(num_hiddens,128, embedding_dim, commitment_cost).to(device)\n",
    "model.load_state_dict(torch.load(f'/home/hiren/Apoorv Pandey/ADRL/Ass1/VQ_VAE_TinyImageNet_{128}.pt'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "valid_originals = next(iter(val_loader))\n",
    "valid_originals = valid_originals.to(device)\n",
    "\n",
    "vq_output_eval = model.pre_vq_conv(model.encoder(valid_originals))\n",
    "_, valid_quantize = model.vq_vae(vq_output_eval)\n",
    "valid_reconstructions = model.decoder(valid_quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Getting latent vectors output as a dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = embedding_dim\n",
    "latent_dim = 512\n",
    "model = Model(num_hiddens,num_embeddings, embedding_dim, commitment_cost).to(device)\n",
    "model.load_state_dict(torch.load(f'/home/hiren/Apoorv Pandey/ADRL/Ass1/VQ_VAE_TinyImageNet_{num_embeddings}.pt'))\n",
    "model.eval()\n",
    "latent_vectors_list = []\n",
    "encoded_vectors_list = []\n",
    "\n",
    "### Getting latent vectors output as a dataset\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)  ## get first batch \n",
    "        vq_loss, data_recon, latent_vectors,encoded = model(data)\n",
    "        \n",
    "        latent_vectors_list.append(latent_vectors.cpu())\n",
    "        encoded_vectors_list.append(encoded.cpu())\n",
    "        torch.cuda.empty_cache()\n",
    "    latent_vectors = torch.cat(latent_vectors_list, dim=0)\n",
    "    encoded_vectors = torch.cat(encoded_vectors_list, dim=0)\n",
    "    latent_vectors_np = latent_vectors.cpu().numpy()\n",
    "    encoded_vectors_np = encoded_vectors.cpu().numpy()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#gm_e = GaussianMixture(n_components=5, random_state=0).fit(encoded_vectors_pca)  ## Fitting a GMM on latent space of quantized outputs\n",
    "in_channels = latent_vectors.size()[1]\n",
    "## Fitting a VAE on latent space\n",
    "\n",
    "train_quantized_dataset = LatentDataset(latent_vectors)\n",
    "train_quantized_dataloader = DataLoader(train_quantized_dataset,batch_size=100,shuffle=True)\n",
    "train_q_variance = get_variance(train_quantized_dataloader)\n",
    "\n",
    "#train_VAE(in_channels,latent_dim,128,train_quantized_dataloader,train_q_variance,f'/home/hiren/Apoorv Pandey/ADRL/Ass1/Latent_VAE_Q_{latent_dim}.pt')  ## Fitting a VAE on latent space of quantized outputs\n",
    "\n",
    "\n",
    "model = VanillaVAE_Conv(in_channels,latent_dim,num_hiddens,in_channels).to(device)\n",
    "\n",
    "train_VAE(model,in_channels,latent_dim,128,train_quantized_dataloader,train_q_variance,f'/home/hiren/Apoorv Pandey/ADRL/Ass1/Latent_VAE_Q_{latent_dim}.pt') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visalize reconstructed images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_latentVAE = VanillaVAE(64,512,128,64).to(device)\n",
    "model_latentVAE.load_state_dict(torch.load(f'/home/hiren/Apoorv Pandey/ADRL/Ass1/Latent_VAE_Q_{latent_dim}.pt'))\n",
    "\n",
    "train_originals = next(iter(train_loader))\n",
    "train_originals = train_originals.to(device)\n",
    "_, _,quantized,encoded = model(train_originals)\n",
    "recons_quantized,_,_,_ = model_latentVAE(quantized)\n",
    "train_reconstruction = model.decoder(recons_quantized)\n",
    "\n",
    "show(make_grid(train_reconstruction.cpu(),nrow=10), )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fitting a GMM on latent space and sampling from it</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "data = next(iter(train_loader)).to(device)\n",
    "vq_loss, data_recon, latent_vectors,encoded = model(data)\n",
    "latent_vectors_np = latent_vectors.detach().cpu().numpy()\n",
    "n = latent_vectors_np.shape[0]\n",
    "gm_q = GaussianMixture(n_components=64, random_state=0).fit(latent_vectors_np.reshape(n,-1)) ## Fitting a GMM on latent space of quantized outputs\n",
    "\n",
    "\n",
    "samples = gm_q.sample(100)## sampling from gmm\n",
    "samples = torch.tensor(samples.reshape(-1,64,16,16),dtype=torch.float)\n",
    "train_reconstruction = model.decoder(samples)\n",
    "\n",
    "show(make_grid(train_reconstruction.cpu(),nrow=10), ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DCGAN</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('conv') != -1:\n",
    "        nn.init.normal_(w.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('bn') != -1:\n",
    "        nn.init.normal_(w.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(w.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitmojiDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.paths = glob(self.root_dir+\"/*\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = self.paths[idx]\n",
    "        image = transform.resize(io.imread(img_name), (64,64,3)).reshape(3,64,64)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_scale_contrast_streching(image):\n",
    "    a = np.min(image)\n",
    "    b = np.max(image)\n",
    "    P = 255/(b-a)\n",
    "    L = -1*P*a\n",
    "    contrast_enhanced_image = (P*image+L).astype('uint8')\n",
    "    return contrast_enhanced_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generator</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input is the latent vector Z.\n",
    "        self.tconv1 = nn.ConvTranspose2d(params['nz'], params['ngf']*8,\n",
    "            kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(params['ngf']*8)\n",
    "\n",
    "        # Input Dimension: (ngf*8) x 4 x 4\n",
    "        self.tconv2 = nn.ConvTranspose2d(params['ngf']*8, params['ngf']*4,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ngf']*4)\n",
    "\n",
    "        # Input Dimension: (ngf*4) x 8 x 8\n",
    "        self.tconv3 = nn.ConvTranspose2d(params['ngf']*4, params['ngf']*2,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ngf']*2)\n",
    "\n",
    "        # Input Dimension: (ngf*2) x 16 x 16\n",
    "        self.tconv4 = nn.ConvTranspose2d(params['ngf']*2, params['ngf'],\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(params['ngf'])\n",
    "\n",
    "        # Input Dimension: (ngf) * 32 * 32\n",
    "        self.tconv5 = nn.ConvTranspose2d(params['ngf'], params['nc'],\n",
    "            4, 2, 1, bias=False)\n",
    "        #Output Dimension: (nc) x 64 x 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.tconv1(x)))\n",
    "        x = F.relu(self.bn2(self.tconv2(x)))\n",
    "        x = F.relu(self.bn3(self.tconv3(x)))\n",
    "        x = F.relu(self.bn4(self.tconv4(x)))\n",
    "\n",
    "        x = F.tanh(self.tconv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Discriminator</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input Dimension: (nc) x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(params['nc'], params['ndf'],\n",
    "            4, 2, 1, bias=False)\n",
    "\n",
    "        # Input Dimension: (ndf) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(params['ndf'], params['ndf']*2,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ndf']*2)\n",
    "\n",
    "        # Input Dimension: (ndf*2) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(params['ndf']*2, params['ndf']*4,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ndf']*4)\n",
    "\n",
    "        # Input Dimension: (ndf*4) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(params['ndf']*4, params['ndf']*8,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(params['ndf']*8)\n",
    "\n",
    "        # Input Dimension: (ndf*8) x 4 x 4\n",
    "        self.conv5 = nn.Conv2d(params['ndf']*8, 1, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, True)\n",
    "\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 369\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "params = {\n",
    "    \"bsize\" : 512,# Batch size during training.\n",
    "    'imsize' : 128,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 3,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 128,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 128, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 10,# Number of training epochs.\n",
    "    'lr' : 0.0002,# Learning rate for optimizers\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'save_epoch' : 2}# Save step.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "data = BitmojiDataset('bitmojis')\n",
    "dataloader = DataLoader(data, params['bsize'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Intialization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(params).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator(params).to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(64, params['nz'], 1, 1, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "print(\"-\"*25)\n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_data = data.float().to(device)\n",
    "        b_size = real_data.size(0)\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        label = torch.full((b_size, ), real_label, device=device).float()\n",
    "        output = netD(real_data).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
    "        fake_data = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake_data).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%50 == 0:\n",
    "            print(torch.cuda.is_available())\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, params['nepochs'], i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        if (iters % 100 == 0) or ((epoch == params['nepochs']-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_data = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake_data, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if epoch % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'model_epoch_{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving plot and models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'model_final.pth')\n",
    "            \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('DCGan models/train_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating images for the grid and saving it</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(100, params['nz'], 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    generated_img = netG(noise).detach().cpu()\n",
    "generated_img = generated_img.numpy().reshape(100,64,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [np.clip(im,0,1) for im in generated_img]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.savefig('DCGan models/grid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = []\n",
    "dims = []\n",
    "for i in range(10):\n",
    "    noise = torch.randn(1,100, 1, 1).to(device)\n",
    "    r = np.random.randint(100)\n",
    "    while(r in dims):\n",
    "        r = np.random.randint(100)\n",
    "    dims.append(r)\n",
    "    temp_noise = noise\n",
    "    for j in np.linspace(0,1,10):\n",
    "        temp_noise[0][r][0][0] = j\n",
    "        generated_images.append(netG(temp_noise).cpu().detach().numpy())\n",
    "generated_images = np.array(generated_images).reshape(100,64,64,3)\n",
    "\n",
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [np.clip(im,0,1) for im in generated_images]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.savefig('DCGan/latent_grid.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>FID Calculations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting 1000 random images\n",
    "image_paths = np.array(glob('bitmojis/*'))\n",
    "image_paths = image_paths[np.random.randint(0, len(image_paths), 1000)]\n",
    "for i in image_paths:\n",
    "    image = transform.resize(io.imread(i), (64,64,3))\n",
    "    io.imsave(\"FID/real/\"+i.split('/')[1], full_scale_contrast_streching(image))\n",
    "\n",
    "#generating 1000 images\n",
    "noise = torch.FloatTensor(np.random.randn(1000,100, 1, 1)).to(device)\n",
    "model = Generator(params)\n",
    "generated_img = netG(noise).cpu().detach().numpy().reshape(1000,64,64,3)\n",
    "for i in range(len(generated_img)):\n",
    "    io.imsave('FID/generated/'+str(i)+'.png',full_scale_contrast_streching(generated_img[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytorch_fid FID/real FID/generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSGAN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, only the loss function is changed, everything is same until Intialization part. Same functions and classed can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Intialization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(params).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator(params).to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "fixed_noise = torch.randn(64, params['nz'], 1, 1, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = -1\n",
    "\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr=params['lr'])#, betas=(params['beta1'], 0.999))\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=params['lr'])#, betas=(params['beta1'], 0.999))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "print(\"-\"*25)\n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_data = data.float().to(device)\n",
    "        b_size = real_data.size(0)\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        label = torch.full((b_size, ), real_label, device=device).float()\n",
    "        output = netD(real_data).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
    "        fake_data = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake_data).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%50 == 0:\n",
    "            print(torch.cuda.is_available())\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, params['nepochs'], i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        if (iters % 100 == 0) or ((epoch == params['nepochs']-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_data = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake_data, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if epoch % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'lsgan/model_epoch_{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'lsgan/model_final.pth')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('lsgan/train_plot_lsgan.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(100, params['nz'], 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    generated_img = netG(noise).detach().cpu()\n",
    "generated_img = generated_img.numpy().reshape(100,64,64,3)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [np.clip(im,0,1) for im in generated_img]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.savefig('lsgan/grid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = []\n",
    "dims = []\n",
    "for i in range(10):\n",
    "    noise = torch.randn(1,100, 1, 1).to(device)\n",
    "    r = np.random.randint(100)\n",
    "    while(r in dims):\n",
    "        r = np.random.randint(100)\n",
    "    dims.append(r)\n",
    "    temp_noise = noise\n",
    "    for j in np.linspace(0,1,10):\n",
    "        temp_noise[0][r][0][0] = j\n",
    "        generated_images.append(netG(temp_noise).cpu().detach().numpy())\n",
    "generated_images = np.array(generated_images).reshape(100,64,64,3)\n",
    "\n",
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [np.clip(im,0,1) for im in generated_images]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.savefig('lsgan/latent_grid.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BiGAN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most things stay the same except that the discriminator accepts 4 channels now instead of 3. And we have a new encoder model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Discriminator</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input Dimension: (nc) x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(params['nc']+1, params['ndf'],\n",
    "            4, 2, 1, bias=False)\n",
    "\n",
    "        # Input Dimension: (ndf) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(params['ndf'], params['ndf']*2,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ndf']*2)\n",
    "\n",
    "        # Input Dimension: (ndf*2) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(params['ndf']*2, params['ndf']*4,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ndf']*4)\n",
    "\n",
    "        # Input Dimension: (ndf*4) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(params['ndf']*4, params['ndf']*8,\n",
    "            4, 1, 0, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(params['ndf']*8)\n",
    "\n",
    "        # Input Dimension: (ndf*8) x 4 x 4\n",
    "        self.conv5 = nn.Conv2d(params['ndf']*8, 1, 4, 2, 0, bias=False)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x = torch.cat((x,z.view(x.shape[0],1,64,64)), dim=1)\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, True)\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Encoder</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, params): \n",
    "        super().__init__()\n",
    "\n",
    "        # Input Dimension: (nc) x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(params['nc'], params['ndf'],\n",
    "            4, 2, 1, bias=False)\n",
    "\n",
    "        # Input Dimension: (ndf) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(params['ndf'], params['ndf']*2,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ndf']*2)\n",
    "\n",
    "        # Input Dimension: (ndf*2) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(params['ndf']*2, params['ndf']*4,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ndf']*4)\n",
    "\n",
    "        # Input Dimension: (ndf*4) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(params['ndf']*4, params['ndf']*8,\n",
    "            4, 1, 0, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(params['ndf']*8)\n",
    "\n",
    "        # Input Dimension: (ndf*8) x 4 x 4\n",
    "        self.conv5 = nn.Conv2d(params['ndf']*8, 32*32, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bsize = x.shape[0]\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, True)\n",
    "        x = F.relu(self.conv5(x))\n",
    "\n",
    "        return x.view(bsize, 1, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters</h3>\n",
    "A few new hyperparameters are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 369\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "params = {\n",
    "    \"bsize\" : 1024,# Batch size during training.\n",
    "    'imsize' : 128,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 3,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 128,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 128, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 5,# Number of training epochs.\n",
    "    'lrg':0.002,\n",
    "    'lrd' : 0.0002,#Learning rate for optimizers\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'save_epoch' : 2,# Save step.\n",
    "    'num_classes':10,#number of classes\n",
    "    'n_descriminator':5,\n",
    "    'label_embeddings':100}#dimension of label embeddings\n",
    "\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "data = BitmojiDataset('bitmojis')\n",
    "dataloader = DataLoader(data, 1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(params).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netE = Encoder(params).to(device)\n",
    "netE.apply(weights_init)\n",
    "\n",
    "netD = Discriminator(params).to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(100, params['nz'],1,1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=params['lrd'], betas=(params['beta1'], 0.999))\n",
    "optimizerG = optim.Adam(list(netE.parameters()) + list(netG.parameters()), lr=params['lrg'], betas=(params['beta1'], 0.999))\n",
    "#optimizerE = optim.Adam(netE.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "E_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "print(\"-\"*25)\n",
    "\n",
    "for epoch in range(params['nepochs'],params['nepochs']*2):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_data = data.float().to(device)\n",
    "        b_size = real_data.size(0)\n",
    "\n",
    "        noise = torch.randn(b_size, params['nz'],1,1, device=device)\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        label = torch.full((b_size,), real_label, device=device).float()\n",
    "        real_z = netE(real_data)\n",
    "        output_real = netD(real_data, real_z).view(-1).to(device)\n",
    "        errD_real = criterion(output_real, label)\n",
    "        errD_real.backward(retain_graph=True)\n",
    "        D_x = output_real.mean().item()\n",
    "        \n",
    "        fake_data = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake_data.detach(), netE(fake_data)).view(-1).to(device)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward(retain_graph=True)\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #if(epoch%params['n_descriminator']==0):\n",
    "\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake_data, netE(fake_data)).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        \n",
    "        optimizerG.step()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        if i%50 == 0:\n",
    "            print(torch.cuda.is_available())\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                    % (epoch+1, params['nepochs'], i, len(dataloader),\n",
    "                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        if (iters % 100 == 0) or ((epoch == params['nepochs']-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_data = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake_data, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if epoch % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'bigan2/model_epoch_{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'bigan2/model_final.pth')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('bigan2/training_plots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(100, params['nz'],1,1, device=device)\n",
    "labels = torch.randint(low = 0, high=10, size=(100,)).to(device)\n",
    "with torch.no_grad():\n",
    "    generated_img = netG(noise).detach().cpu()\n",
    "generated_img = generated_img.numpy().reshape(100,64,64,3)\n",
    "\n",
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [np.clip(im,0,1) for im in generated_img]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "plt.savefig('bigan2/grid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = []\n",
    "dims = []\n",
    "for i in range(10):\n",
    "    noise = torch.randn(1,100, 1, 1).to(device)\n",
    "    r = np.random.randint(100)\n",
    "    while(r in dims):\n",
    "        r = np.random.randint(100)\n",
    "    dims.append(r)\n",
    "    temp_noise = noise\n",
    "    for j in np.linspace(0,1,10):\n",
    "        temp_noise[0][r][0][0] = j\n",
    "        generated_images.append(netG(temp_noise).cpu().detach().numpy())\n",
    "generated_images = np.array(generated_images).reshape(100,64,64,3)\n",
    "\n",
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [np.clip(im,0,1) for im in generated_images]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.savefig('bigan2/latent_grid.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>WGAN</h1><h4>(EXTRA, using BitMoji dataset)</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everthing stays the same except discriminator has tanh output activation now and using RMSprop as optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gradient penalty calculation function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_penalty(model, real_images, fake_images, device):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake data\n",
    "    alpha = torch.randn((real_images.size(0), 1, 1, 1), device=device)\n",
    "    # Get random interpolation between real and fake data\n",
    "    interpolates = (alpha * real_images + ((1 - alpha) * fake_images)).requires_grad_(True)\n",
    "\n",
    "    model_interpolates = model(interpolates)\n",
    "    grad_outputs = torch.ones(model_interpolates.size(), device=device, requires_grad=False)\n",
    "\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=model_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = torch.mean((gradients.norm(2, dim=1) - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Discriminator</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input Dimension: (nc) x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(params['nc'], params['ndf'],\n",
    "            4, 2, 1, bias=False)\n",
    "\n",
    "        # Input Dimension: (ndf) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(params['ndf'], params['ndf']*2,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ndf']*2)\n",
    "\n",
    "        # Input Dimension: (ndf*2) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(params['ndf']*2, params['ndf']*4,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ndf']*4)\n",
    "\n",
    "        # Input Dimension: (ndf*4) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(params['ndf']*4, params['ndf']*8,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(params['ndf']*8)\n",
    "\n",
    "        # Input Dimension: (ndf*8) x 4 x 4\n",
    "        self.conv5 = nn.Conv2d(params['ndf']*8, 1, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, True)\n",
    "\n",
    "        x = F.tanh(self.conv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(params).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator(params).to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(64, params['nz'], 1, 1, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "print(\"-\"*25)\n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_data = data.float().to(device)\n",
    "        b_size = real_data.size(0)\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        label = torch.full((b_size, ), real_label, device=device).float()\n",
    "        output = netD(real_data).view(-1)\n",
    "        errD_real = torch.mean(output)\n",
    "        #errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
    "        fake_data = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        errD_fake = torch.mean(output)\n",
    "        #errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        #gradient_penalty = calculate_gradient_penalty(netD, real_data, fake_data, device)\n",
    "        errD = -errD_real + errD_fake #+ gradient_penalty * 10\n",
    "        errD.backward()    \n",
    "        optimizerD.step()\n",
    "        for p in netD.parameters():\n",
    "                p.data.clamp_(-params['clip'], params['clip'])\n",
    "        \n",
    "        if(epoch%params['n_descriminator']==0):\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)\n",
    "            output = netD(fake_data).view(-1)\n",
    "            errG = -torch.mean(output)\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            D_G_z2 = output.mean().item()\n",
    "\n",
    "            if i%50 == 0:\n",
    "                print(torch.cuda.is_available())\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, params['nepochs'], i, len(dataloader),\n",
    "                         errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if epoch % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'WGan models/model_epoch_{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'wgan/model_final.pth')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('wgan/training_plots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(100, params['nz'],1,1, device=device)\n",
    "labels = torch.randint(low = 0, high=10, size=(100,)).to(device)\n",
    "with torch.no_grad():\n",
    "    generated_img = netG(noise).detach().cpu()\n",
    "generated_img = generated_img.numpy().reshape(100,64,64,3)\n",
    "\n",
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [np.clip(im,0,1) for im in generated_img]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "plt.savefig('wgan/grid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = []\n",
    "dims = []\n",
    "for i in range(10):\n",
    "    noise = torch.randn(1,100, 1, 1).to(device)\n",
    "    r = np.random.randint(100)\n",
    "    while(r in dims):\n",
    "        r = np.random.randint(100)\n",
    "    dims.append(r)\n",
    "    temp_noise = noise\n",
    "    for j in np.linspace(0,1,10):\n",
    "        temp_noise[0][r][0][0] = j\n",
    "        generated_images.append(netG(temp_noise).cpu().detach().numpy())\n",
    "generated_images = np.array(generated_images).reshape(100,64,64,3)\n",
    "\n",
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [np.clip(im,0,1) for im in generated_images]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.savefig('wgan/latent_grid.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conditional DCGAN</h1><h4>(Extra, didn't get perceptually good results)</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>DataLoader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svnhDataLoader(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        X_train = np.load(root_dir+'/X_train.npy')\n",
    "        y_train = np.load(root_dir+'/y_train.npy')\n",
    "        X_test = np.load(root_dir+'/X_test.npy')\n",
    "        y_test = np.load(root_dir+'/y_test.npy')\n",
    "        self.X = np.concatenate([X_train, X_test], axis=3)\n",
    "        self.X = self.X.reshape(99289, 3, 32, 32)\n",
    "        self.y = np.append(y_train, y_test)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generator</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(params['num_classes'], params['label_embeddings'])\n",
    "        # Input is the latent vector Z.\n",
    "        self.tconv1 = nn.ConvTranspose2d(params['nz']+params['label_embeddings'], params['ngf']*8,\n",
    "            kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(params['ngf']*8)\n",
    "\n",
    "        # Input Dimension: (ngf*8) x 4 x 4\n",
    "        self.tconv2 = nn.ConvTranspose2d(params['ngf']*8, params['ngf']*4,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ngf']*4)\n",
    "\n",
    "        # Input Dimension: (ngf*4) x 8 x 8\n",
    "        self.tconv3 = nn.ConvTranspose2d(params['ngf']*4, params['ngf']*2,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ngf']*2)\n",
    "\n",
    "        # Input Dimension: (ngf*2) x 16 x 16\n",
    "        self.tconv4 = nn.ConvTranspose2d(params['ngf']*2, params['nc'],\n",
    "            4, 2, 1, bias=False)\n",
    "#        self.bn4 = nn.BatchNorm2d(params['ngf'])\n",
    "\n",
    "        # Input Dimension: (ngf) * 32 * 32\n",
    "        #self.tconv5 = nn.ConvTranspose2d(params['ngf'], params['nc'],\n",
    "            #4, 2, 1, bias=False)\n",
    "        #Output Dimension: (nc) x 64 x 64\n",
    "\n",
    "    def forward(self, z,y):\n",
    "        y=self.embed(y.long())\n",
    "        z=torch.cat([z,y],1)\n",
    "        z=z.view(-1,params['nz']+params['label_embeddings'],1,1)\n",
    "        x = F.relu(self.bn1(self.tconv1(z)))\n",
    "        x = F.relu(self.bn2(self.tconv2(x)))\n",
    "        x = F.relu(self.bn3(self.tconv3(x)))\n",
    "        #x = F.relu(self.bn4(self.tconv4(x)))\n",
    "\n",
    "        x = F.tanh(self.tconv4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Discriminator</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input Dimension: (nc) x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(params['nc']+1, params['ndf'],\n",
    "            4, 2, 1, bias=False)\n",
    "\n",
    "        # Input Dimension: (ndf) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(params['ndf'], params['ndf']*2,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ndf']*2)\n",
    "\n",
    "        # Input Dimension: (ndf*2) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(params['ndf']*2, params['ndf']*4,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ndf']*4)\n",
    "\n",
    "        # Input Dimension: (ndf*4) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(params['ndf']*4, 1,\n",
    "            4, 1, 0, bias=False)\n",
    "        #self.bn4 = nn.BatchNorm2d(params['ndf']*8)\n",
    "\n",
    "        # Input Dimension: (ndf*8) x 4 x 4\n",
    "        #self.conv5 = nn.Conv2d(params['ndf']*8, 1, 4, 1, 0, bias=False)\n",
    "        self.embed = nn.Embedding(params['num_classes'],32*32)\n",
    "\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels.long()).view(labels.shape[0], 1, 32,32)\n",
    "        x = torch.cat([x,embedding], dim=1)\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, True)\n",
    "#        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, True)\n",
    "\n",
    "        x = F.sigmoid(self.conv4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 369\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "params = {\n",
    "    \"bsize\" : 512,# Batch size during training.\n",
    "    'imsize' : 128,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 3,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 128,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 128, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 25,# Number of training epochs.\n",
    "    'lrg' : 0.002,#Learning rate for generator\n",
    "    'lrd' : 0.00002,#Learning rate for discriminator\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'save_epoch' : 2,# Save step.\n",
    "    'num_classes':10,#number of classes\n",
    "    'n_descriminator':5,\n",
    "    'label_embeddings':50}#dimension of label embeddings\n",
    "\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "data = svnhDataLoader('svnh')\n",
    "dataloader = DataLoader(data, 512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(params).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator(params).to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(100, params['nz'], device=device)\n",
    "fixed_labels = torch.randint(low = 0, high=10, size=(100,), device= device) #to_categorical(torch.randint(low = 0, high=10, size=(100,)).cpu().detach(), params['num_classes']).to(device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=params['lrd'], betas=(params['beta1'], 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=params['lrg'], betas=(params['beta1'], 0.999))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "print(\"-\"*25)\n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_data,labels = data[0].float().to(device),data[1].float().to(device)#to_categorical(data[1].cpu().detach(), params['num_classes']).to(device)\n",
    "        b_size = real_data.size(0)\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        label = torch.full((b_size,), real_label, device=device).float()\n",
    "        output = netD(real_data, labels).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        noise = torch.randn(b_size, params['nz'], device=device)\n",
    "        fake_data = netG(noise, labels)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake_data.detach(), labels).view(-1).to(device)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #if(epoch%params['n_descriminator']==0):\n",
    "\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake_data, labels).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%50 == 0:\n",
    "            print(torch.cuda.is_available())\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                    % (epoch, params['nepochs'], i, len(dataloader),\n",
    "                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        if (iters % 100 == 0) or ((epoch == params['nepochs']-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_data = netG(fixed_noise, fixed_labels).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake_data, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if epoch % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'cDCGAN outputs/model_epoch_{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'cDCGAN outputs/model_final.pth')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('cDCGAN outputs/training_plots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(100, params['nz'], device=device)\n",
    "labels = torch.randint(low = 0, high=10, size=(100,)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_img = netG(noise, labels).detach().cpu()\n",
    "\n",
    "generated_img = generated_img.numpy().reshape(100,32,32,3)\n",
    "\n",
    "fig = plt.figure(figsize=(200., 200.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [np.clip(im,0,1) for im in generated_img]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('cDCGAN outputs/grid.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('adrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cabdae8c392c33407f81be0fa6b0265a7862a0aa79c86b5fc5ae294120605a2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
